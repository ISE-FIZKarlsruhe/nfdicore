{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NFDI core ontology Welcome to the documentation page for our ontology developed within the framework of the National Research Data Infrastructure (NFDI) initiative . The NFDI initiative is dedicated to making scientific and research data available, interconnected, and sustainable for long-term use. The NFDI brings together institutions from various research disciplines to collaborate on essential services, training opportunities for researchers, and the development of standards for data management. The overarching vision of NFDI is to treat data as a shared asset for excellent research, managed by the scientific community in Germany. The NFDIcore ontology captures both the structure of the NFDI organization and the diverse datasets provided by NFDI project partners. As a mid-level ontology, it bridges high-level concepts with more specific domain ontologies, ensuring both flexibility and consistency in representing and managing research data. Here you will find information about: introduction and scope of the ontology different versions of the ontology key aspects and modeling descisions information about modular extensions how to contribute","title":"Welcome"},{"location":"#nfdi-core-ontology","text":"Welcome to the documentation page for our ontology developed within the framework of the National Research Data Infrastructure (NFDI) initiative . The NFDI initiative is dedicated to making scientific and research data available, interconnected, and sustainable for long-term use. The NFDI brings together institutions from various research disciplines to collaborate on essential services, training opportunities for researchers, and the development of standards for data management. The overarching vision of NFDI is to treat data as a shared asset for excellent research, managed by the scientific community in Germany. The NFDIcore ontology captures both the structure of the NFDI organization and the diverse datasets provided by NFDI project partners. As a mid-level ontology, it bridges high-level concepts with more specific domain ontologies, ensuring both flexibility and consistency in representing and managing research data. Here you will find information about: introduction and scope of the ontology different versions of the ontology key aspects and modeling descisions information about modular extensions how to contribute","title":"NFDI core ontology"},{"location":"cite/","text":"How to cite NFDICORE","title":"How to cite NFDICORE"},{"location":"cite/#how-to-cite-nfdicore","text":"","title":"How to cite NFDICORE"},{"location":"contact/","text":"How to contact the NFDIcore Team Every Thursday at 10:00 the NFDIcore team meets and discusses the development of the ontology and its extensions in the NFDIcore Playground. Questions about specific modeling decisions can be discussed in the NFDIcore Discussion Forum . Github issues can be used to report errors or to request additional classes and properties. For general questions about NFDIcore and for participation in the Playground, please email Tabea Tietz . Conferences and Events The NFDIcore team can frequently be found at Semantic Web and AI conferences and Research Data Management events. Feel free to find us there and talk to us. Upcoming Events International Conference on Formal Ontology in Information Systems (FOIS), Catania, Italy NFDIxCS Symposium, Potsdam, Germany European Congress and Exhibition on Advanced Materials and Processes (EUROMAT), Granada, Spain NFDI4Culture Community Plenary, Mainz, Germany International Semantic Web Conference (ISWC), Nara, Japan Ontologies4Chem - International Workshop Series, Limburg an der Lahn, Germany","title":"Contact"},{"location":"contact/#how-to-contact-the-nfdicore-team","text":"Every Thursday at 10:00 the NFDIcore team meets and discusses the development of the ontology and its extensions in the NFDIcore Playground. Questions about specific modeling decisions can be discussed in the NFDIcore Discussion Forum . Github issues can be used to report errors or to request additional classes and properties. For general questions about NFDIcore and for participation in the Playground, please email Tabea Tietz .","title":"How to contact the NFDIcore Team"},{"location":"contact/#conferences-and-events","text":"The NFDIcore team can frequently be found at Semantic Web and AI conferences and Research Data Management events. Feel free to find us there and talk to us.","title":"Conferences and Events"},{"location":"contact/#upcoming-events","text":"International Conference on Formal Ontology in Information Systems (FOIS), Catania, Italy NFDIxCS Symposium, Potsdam, Germany European Congress and Exhibition on Advanced Materials and Processes (EUROMAT), Granada, Spain NFDI4Culture Community Plenary, Mainz, Germany International Semantic Web Conference (ISWC), Nara, Japan Ontologies4Chem - International Workshop Series, Limburg an der Lahn, Germany","title":"Upcoming Events"},{"location":"contributing/","text":"How to contribute to NFDIcore Your input helps improve the NFDI Ontology! Here\u2019s how you can contribute: 1. Join the Discussion Check the Discussions tab on GitHub. Share ideas, ask questions, and suggest improvements. 2. Report Issues Found an error or need a new concept? Go to Issues > New Issue , describe the problem, and suggest a solution. 3. Suggest Changes Discuss your idea first in Discussions or Issues . Maintainers will review and help implement improvements. 4. Stay Involved Follow updates and give feedback on new changes. \ud83d\ude80 Every contribution helps make the NFDI Ontology better!","title":"Contributing"},{"location":"contributing/#how-to-contribute-to-nfdicore","text":"Your input helps improve the NFDI Ontology! Here\u2019s how you can contribute:","title":"How to contribute to NFDIcore"},{"location":"contributing/#1-join-the-discussion","text":"Check the Discussions tab on GitHub. Share ideas, ask questions, and suggest improvements.","title":"1. Join the Discussion"},{"location":"contributing/#2-report-issues","text":"Found an error or need a new concept? Go to Issues > New Issue , describe the problem, and suggest a solution.","title":"2. Report Issues"},{"location":"contributing/#3-suggest-changes","text":"Discuss your idea first in Discussions or Issues . Maintainers will review and help implement improvements.","title":"3. Suggest Changes"},{"location":"contributing/#4-stay-involved","text":"Follow updates and give feedback on new changes. \ud83d\ude80 Every contribution helps make the NFDI Ontology better!","title":"4. Stay Involved"},{"location":"extensions/","text":"Modular Extensions To meet the specific needs of individual domains, NFDIcore is designed to be extended by domain and application ontologies. These ontologies build upon NFDIcore's shared structure while providing the expressivity required for their respective communities. In regular community exchanges, it is evaluated which concepts from domain-specific extensions are suitable to be represented as part of the shared mid-level layer and which elements remain domain and function specific. Any consortium and domain is welcome to provide their own extension to NFDIcore. The currently published extensions are listed below. NFDI-MatWerk Ontology (MWO) MWO captures key aspects of MSE research data, including community structures, projects, resources, and services within NFDI-MatWerk. It provides the foundation for the MSE Knowledge Graph , enabling efficient data integration and retrieval, and supporting collaboration and knowledge representation across materials science and engineering. Current version: 3.0 MWO documentation Contact person: Hossein Beygi Nasrabadi NFDI4Culture Ontology (CTO) CTO represents the research data of the NFDI4Culture community within a centralized research data index, providing a single point of access to decentralized cultural heritage research resources. The ontology enables the integration of research (meta)data into the NFDI4Culture Knowledge Graph and, consequently, the NFDI4Culture Information Portal . Current version: 3.0 CTO documentation Contact person: Tabea Tietz NFDI4Memory Ontology (MEMO) MemO represents domain-specific concepts from the historical sciences, focusing on metadata harmonization and detailed provenance representation. It forms the basis of the NFDI4Memory Knowledge Graph , which serves as a central index for research data, institutions, researchers, and services. Current version: 1.0 MemO documentation Contact person: Sarah Rebecca Ondraszek NFDI4DataScience Ontology (DSAI) The vision of NFDI4DataScience (DSAI) is to support all steps of the complex and interdisciplinary research data lifecycle in Data Science and Artificial Intelligence. Current Version: 1.0 DSAI documentation Contact person: Genet Asefa Gesese","title":"Modular Extensions"},{"location":"extensions/#modular-extensions","text":"To meet the specific needs of individual domains, NFDIcore is designed to be extended by domain and application ontologies. These ontologies build upon NFDIcore's shared structure while providing the expressivity required for their respective communities. In regular community exchanges, it is evaluated which concepts from domain-specific extensions are suitable to be represented as part of the shared mid-level layer and which elements remain domain and function specific. Any consortium and domain is welcome to provide their own extension to NFDIcore. The currently published extensions are listed below.","title":"Modular Extensions"},{"location":"extensions/#nfdi-matwerk-ontology-mwo","text":"MWO captures key aspects of MSE research data, including community structures, projects, resources, and services within NFDI-MatWerk. It provides the foundation for the MSE Knowledge Graph , enabling efficient data integration and retrieval, and supporting collaboration and knowledge representation across materials science and engineering. Current version: 3.0 MWO documentation Contact person: Hossein Beygi Nasrabadi","title":"NFDI-MatWerk Ontology (MWO)"},{"location":"extensions/#nfdi4culture-ontology-cto","text":"CTO represents the research data of the NFDI4Culture community within a centralized research data index, providing a single point of access to decentralized cultural heritage research resources. The ontology enables the integration of research (meta)data into the NFDI4Culture Knowledge Graph and, consequently, the NFDI4Culture Information Portal . Current version: 3.0 CTO documentation Contact person: Tabea Tietz","title":"NFDI4Culture Ontology (CTO)"},{"location":"extensions/#nfdi4memory-ontology-memo","text":"MemO represents domain-specific concepts from the historical sciences, focusing on metadata harmonization and detailed provenance representation. It forms the basis of the NFDI4Memory Knowledge Graph , which serves as a central index for research data, institutions, researchers, and services. Current version: 1.0 MemO documentation Contact person: Sarah Rebecca Ondraszek","title":"NFDI4Memory Ontology (MEMO)"},{"location":"extensions/#nfdi4datascience-ontology-dsai","text":"The vision of NFDI4DataScience (DSAI) is to support all steps of the complex and interdisciplinary research data lifecycle in Data Science and Artificial Intelligence. Current Version: 1.0 DSAI documentation Contact person: Genet Asefa Gesese","title":"NFDI4DataScience Ontology (DSAI)"},{"location":"intro/","text":"Introduction The National Research Data Infrastructure (NFDI) initiative initiative has led to the formation of various consortia, each focused on developing a research data infrastructure tailored to its specific domain. To ensure interoperability across these consortia, the NFDIcore ontology has been developed as a mid-level ontology for representing metadata related to NFDI resources, including individuals, organizations, projects, data portals, and more. In line with NFDI's goals, this ontology has been created to provide a structured framework that enables efficient management, organization, and interconnection of research data across various disciplines. By adhering to established data standards, the ontology facilitates the accessibility, sharing, and reuse of research data in a consistent and sustainable manner. The key features of the NFDIcore ontology are: Representation of the NFDI Structure: The ontology models the organizational structure of NFDI, including its components, projects, services, and partnerships. Referencing Partner Data: It enables the description and reference of data from various NFDI project partners, providing a unified framework for organizing and linking research data across disciplines. Standardization and Interoperability: By adhering to data management standards, the ontology ensures that data from different sources can be interconnected, shared, and reused. Recognizing the diverse needs of consortia, NFDIcore is built upon the Basic Formal Ontology (BFO) , a foundational framework essential for enhancing knowledge representation, data exchange, and interdisciplinary collaboration. To address domain-specific research questions, NFDIcore serves as the basis for various application and domain ontologies, which extend its core structure in a modular fashion. Examples include the NFDI4Culture Ontology (CTO) , NFDI MatWerk Ontology (MWO) , NFDI4Memory Ontology (MEMO) , and NFDI4DataScience Ontology (DSO) , each tailored to specific research fields while ensuring semantic interoperability. As a mid-level ontology, NFDIcore plays a central role in structuring and integrating research data across consortia. It provides a shared vocabulary that represents both the organizational structure of the NFDI and the diverse datasets contributed by project partners. The ontology encompasses key concepts such as organizations, consortia, projects, datasets, research outputs, geographical locations, and technical standards. These structured representations enable efficient data management, integration, and reuse across disciplines. NFDIcore focuses on describing the following main concepts: Organizational & Research Structure \u2013 Represents institutions, consortia, projects, researchers, and their roles within NFDI. Data & Information Management \u2013 Covers datasets, data portals, collections, publications, and intellectual outputs to support research. Geographical & Contextual Information \u2013 Includes cities, countries, and regions relevant to NFDI activities. Technology & Standards \u2013 Encompasses software, programming languages, specifications, standards, and ontologies for interoperability. Services & Licensing \u2013 Defines resources, service processes, licenses, and web-based platforms supporting data sharing and collaboration.","title":"Introduction"},{"location":"intro/#introduction","text":"The National Research Data Infrastructure (NFDI) initiative initiative has led to the formation of various consortia, each focused on developing a research data infrastructure tailored to its specific domain. To ensure interoperability across these consortia, the NFDIcore ontology has been developed as a mid-level ontology for representing metadata related to NFDI resources, including individuals, organizations, projects, data portals, and more. In line with NFDI's goals, this ontology has been created to provide a structured framework that enables efficient management, organization, and interconnection of research data across various disciplines. By adhering to established data standards, the ontology facilitates the accessibility, sharing, and reuse of research data in a consistent and sustainable manner. The key features of the NFDIcore ontology are: Representation of the NFDI Structure: The ontology models the organizational structure of NFDI, including its components, projects, services, and partnerships. Referencing Partner Data: It enables the description and reference of data from various NFDI project partners, providing a unified framework for organizing and linking research data across disciplines. Standardization and Interoperability: By adhering to data management standards, the ontology ensures that data from different sources can be interconnected, shared, and reused. Recognizing the diverse needs of consortia, NFDIcore is built upon the Basic Formal Ontology (BFO) , a foundational framework essential for enhancing knowledge representation, data exchange, and interdisciplinary collaboration. To address domain-specific research questions, NFDIcore serves as the basis for various application and domain ontologies, which extend its core structure in a modular fashion. Examples include the NFDI4Culture Ontology (CTO) , NFDI MatWerk Ontology (MWO) , NFDI4Memory Ontology (MEMO) , and NFDI4DataScience Ontology (DSO) , each tailored to specific research fields while ensuring semantic interoperability. As a mid-level ontology, NFDIcore plays a central role in structuring and integrating research data across consortia. It provides a shared vocabulary that represents both the organizational structure of the NFDI and the diverse datasets contributed by project partners. The ontology encompasses key concepts such as organizations, consortia, projects, datasets, research outputs, geographical locations, and technical standards. These structured representations enable efficient data management, integration, and reuse across disciplines. NFDIcore focuses on describing the following main concepts: Organizational & Research Structure \u2013 Represents institutions, consortia, projects, researchers, and their roles within NFDI. Data & Information Management \u2013 Covers datasets, data portals, collections, publications, and intellectual outputs to support research. Geographical & Contextual Information \u2013 Includes cities, countries, and regions relevant to NFDI activities. Technology & Standards \u2013 Encompasses software, programming languages, specifications, standards, and ontologies for interoperability. Services & Licensing \u2013 Defines resources, service processes, licenses, and web-based platforms supporting data sharing and collaboration.","title":"Introduction"},{"location":"ontology/","text":"Ontology The NFDI Ontology is a mid-level ontology designed to represent both the organizational structure of the Nationale Forschungsdateninfrastruktur (NFDI) and the diverse datasets provided by NFDI project partners. Overview The ontology provides a shared vocabulary and a consistent framework that supports data management, integration, and interoperability across a wide range of disciplines. Below is an overview of key concepts within the ontology. The complete list of concepts can be found in the generated ontology description: https://nfdi.fiz-karlsruhe.de/ontology/ . Key Concepts Organizational Organization : Represents institutions, research groups, or companies involved in NFDI activities. Person : An individual, such as a researcher or contributor, participating in NFDI projects. Consortium : Represents collaborative groups formed by multiple organizations within the NFDI. ConsortiumMember : A participant within a consortium, whether an organization or person. ConsortiumMemberRole : Defines the roles played by members within a consortium, such as \"lead partner\" or \"contributor\" Project : Represents individual research projects or initiatives within the NFDI ecosystem. Research and Academic AcademicDiscipline : Categories representing different fields of study or research disciplines. Publication : Scholarly works or research reports published as a result of NFDI projects. Contributing : Represents contributions made by people or organizations to a project or dataset. Event : Represents events such as conferences, workshops, or meetings organized by NFDI or related entities. Geographical City : Represents cities related to projects, events, or organizational locations. Country : Countries where NFDI organizations or research activities are based. FederalState : States or provinces within a country, relevant to the localization of activities or organizations. Place : A more general class representing any location such as cities, countries, or regions. Data and Information Dataset : Structured collections of data produced or managed by NFDI partners. Data Portal : Digital platforms or repositories where datasets are stored and accessed. Collection : Represents grouped resources, such as datasets, publications, or software. CreativeWork : Any intellectual output, including publications, datasets, software, or media. Technology and Standards Software : Represents software tools or applications developed and used within the NFDI. Programming Language : Programming languages used in developing software or tools. Specification : Defines technical specifications related to datasets, software, or services. Standard : Represents standards followed for data management, storage, and dissemination. Technological Method Specification : A technological method employed for the creation, management, utilization, or control of a digital resource. Ontology : Formalized structures of knowledge representation, including the NFDI Ontology itself. NFDI Resource and Service NFDI Resource : General class for any entity utilized within the NFDI framework, such as datasets or software. Service Process : Organizes activities and resources to deliver a specific service, ensuring its efficiency, reliability, and alignment with intended goals. Service Product : Provides value through a service offering, delivering intangible benefits or functionality designed to fulfill specific needs or requirements in a structured process. License : Represents the legal terms under which datasets, publications, or software are made available. Website : Represents websites or web services associated with NFDI partners or projects. Formats MIME-Type : Serves as a standardized identifier for specifying the nature and format of data. Export Format Specification : Defines the structural, syntactic, and encoding rules for representing data when it is exported from a system. Class Hierarchy The image below illustrates the top-level hierarchy of the ontology (Version 3.0.0). Most NFDIcore classes have been defined as subclasses of Basic Formal Ontology (BFO), Information Artifact Ontology (IAO), or the Software Ontology (SWO). Since BFO 2020 \u2014 the version currently adopted by NFDIcore \u2014 is not fully supported by IAO, some relevant IAO concepts could not be reused. In such cases, NFDIcore-specific classes have been introduced, such as dataset, document and identifier. BFO as top level ontology The Basic Formal Ontology (BFO) is a top-level ontology that provides a structured framework for organizing entities based on their fundamental nature. It does not include domain-specific content but instead defines high-level categories that support the development of specialized ontologies like NFDIcore. BFO distinguishes entities based on whether they persist through time or unfold over time, dividing them into continuants and occurrents (see image above). Continuants (Endurants) Continuants are entities that exist at any given moment in time and maintain their identity over time. There are tree kinds of continuants: independent continuants, generically dependent continuants and specifically dependent continuants. Independent Continuants (IC) These are entities that exist independently and do not require another entity to exist. Material Entities \u2013 Physical objects with spatial extension. Examples : Organisms, buildings, tools. Immaterial Entities \u2013 Boundaries or parts of objects defined by human convention. Examples : The equator, the upper half of a sphere. Generically Dependent Continuants (GDC) These entities depend on independent continuants for their existence. Generically dependent continuants can exist in multiple instances or be replicated across different locations. Examples : A book\u2019s content (as opposed to a single physical copy of the book) A software program (which can be installed on multiple computers) A musical composition (which can be played on different instruments) A dataset and data items Entities with information content Specifically Dependent Continuants (SDC) Specifically dependent continuants are qualities, roles, or dispositions that exist only in relation to a particular independent continuant . They cannot exist independently and must always be inherent in something else . Qualities - Intrinsic properties of an independent continuant. They describe how an entity is at any moment in time. Examples : The color of a leaf, the weight of a person, the temperature of a liquid. Roles - Situational properties that an entity has based on context or social convention . Examples : The role of a teacher, he status of a patient in a hospital, the role of a machine undergoing maintenance. Dispositions and functions - Potential behaviors or tendencies that an entity has, even if they are not currently being realized. Functions are dispositions that represent the particular purpose of something. Examples : The fragility of glass (it might break if dropped), the solubility of salt (it dissolves in water), a person\u2019s ability to speak multiple languages, the function of an oven to heat something up, the function of a screwdriver to turn screws in and out. Occurrents (Perdurants) An occurrents is an entity that unfolds itself in time or it is the start or end of such an entity. Processes Processes are dynamic activities with temporal duration. Examples : A running event, a chemical reaction, cell division. Temporal Regions These represent divisions of time. Examples : A second, an hour, a historical period. Spatiotemporal Regions These combine space and time into a single entity. Examples : The path of a moving object, the trajectory of a planet. Relations in BFO BFO defines formal relationships between entities to maintain consistency. Some key relations include: continunat part of \u2013 Indicates compositional relationships. ( Example: A wheel is part of a car. ) occurent part of - Some process has another process as part. ( Example: A conference event has multiple workshop events. ) located in \u2013 Specifies spatial containment. ( Example: A book is located_in a library. ) bearer of \u2013 Assigns specifically dependent continuants to independent continuants. ( Example: A teacher is the bearer of the educator role. ) has participant - Assigns continuants to processes. ( Example: A student participates a lecture event. ) More information about BFO can be found at the GitHub repo and the documentation page .","title":"Ontology"},{"location":"ontology/#ontology","text":"The NFDI Ontology is a mid-level ontology designed to represent both the organizational structure of the Nationale Forschungsdateninfrastruktur (NFDI) and the diverse datasets provided by NFDI project partners.","title":"Ontology"},{"location":"ontology/#overview","text":"The ontology provides a shared vocabulary and a consistent framework that supports data management, integration, and interoperability across a wide range of disciplines. Below is an overview of key concepts within the ontology. The complete list of concepts can be found in the generated ontology description: https://nfdi.fiz-karlsruhe.de/ontology/ .","title":"Overview"},{"location":"ontology/#key-concepts","text":"","title":"Key Concepts"},{"location":"ontology/#organizational","text":"Organization : Represents institutions, research groups, or companies involved in NFDI activities. Person : An individual, such as a researcher or contributor, participating in NFDI projects. Consortium : Represents collaborative groups formed by multiple organizations within the NFDI. ConsortiumMember : A participant within a consortium, whether an organization or person. ConsortiumMemberRole : Defines the roles played by members within a consortium, such as \"lead partner\" or \"contributor\" Project : Represents individual research projects or initiatives within the NFDI ecosystem.","title":"Organizational"},{"location":"ontology/#research-and-academic","text":"AcademicDiscipline : Categories representing different fields of study or research disciplines. Publication : Scholarly works or research reports published as a result of NFDI projects. Contributing : Represents contributions made by people or organizations to a project or dataset. Event : Represents events such as conferences, workshops, or meetings organized by NFDI or related entities.","title":"Research and Academic"},{"location":"ontology/#geographical","text":"City : Represents cities related to projects, events, or organizational locations. Country : Countries where NFDI organizations or research activities are based. FederalState : States or provinces within a country, relevant to the localization of activities or organizations. Place : A more general class representing any location such as cities, countries, or regions.","title":"Geographical"},{"location":"ontology/#data-and-information","text":"Dataset : Structured collections of data produced or managed by NFDI partners. Data Portal : Digital platforms or repositories where datasets are stored and accessed. Collection : Represents grouped resources, such as datasets, publications, or software. CreativeWork : Any intellectual output, including publications, datasets, software, or media.","title":"Data and Information"},{"location":"ontology/#technology-and-standards","text":"Software : Represents software tools or applications developed and used within the NFDI. Programming Language : Programming languages used in developing software or tools. Specification : Defines technical specifications related to datasets, software, or services. Standard : Represents standards followed for data management, storage, and dissemination. Technological Method Specification : A technological method employed for the creation, management, utilization, or control of a digital resource. Ontology : Formalized structures of knowledge representation, including the NFDI Ontology itself.","title":"Technology and Standards"},{"location":"ontology/#nfdi-resource-and-service","text":"NFDI Resource : General class for any entity utilized within the NFDI framework, such as datasets or software. Service Process : Organizes activities and resources to deliver a specific service, ensuring its efficiency, reliability, and alignment with intended goals. Service Product : Provides value through a service offering, delivering intangible benefits or functionality designed to fulfill specific needs or requirements in a structured process. License : Represents the legal terms under which datasets, publications, or software are made available. Website : Represents websites or web services associated with NFDI partners or projects.","title":"NFDI Resource and Service"},{"location":"ontology/#formats","text":"MIME-Type : Serves as a standardized identifier for specifying the nature and format of data. Export Format Specification : Defines the structural, syntactic, and encoding rules for representing data when it is exported from a system.","title":"Formats"},{"location":"ontology/#class-hierarchy","text":"The image below illustrates the top-level hierarchy of the ontology (Version 3.0.0). Most NFDIcore classes have been defined as subclasses of Basic Formal Ontology (BFO), Information Artifact Ontology (IAO), or the Software Ontology (SWO). Since BFO 2020 \u2014 the version currently adopted by NFDIcore \u2014 is not fully supported by IAO, some relevant IAO concepts could not be reused. In such cases, NFDIcore-specific classes have been introduced, such as dataset, document and identifier.","title":"Class Hierarchy"},{"location":"ontology/#bfo-as-top-level-ontology","text":"The Basic Formal Ontology (BFO) is a top-level ontology that provides a structured framework for organizing entities based on their fundamental nature. It does not include domain-specific content but instead defines high-level categories that support the development of specialized ontologies like NFDIcore. BFO distinguishes entities based on whether they persist through time or unfold over time, dividing them into continuants and occurrents (see image above).","title":"BFO as top level ontology"},{"location":"ontology/#continuants-endurants","text":"Continuants are entities that exist at any given moment in time and maintain their identity over time. There are tree kinds of continuants: independent continuants, generically dependent continuants and specifically dependent continuants.","title":"Continuants (Endurants)"},{"location":"ontology/#independent-continuants-ic","text":"These are entities that exist independently and do not require another entity to exist. Material Entities \u2013 Physical objects with spatial extension. Examples : Organisms, buildings, tools. Immaterial Entities \u2013 Boundaries or parts of objects defined by human convention. Examples : The equator, the upper half of a sphere.","title":"Independent Continuants (IC)"},{"location":"ontology/#generically-dependent-continuants-gdc","text":"These entities depend on independent continuants for their existence. Generically dependent continuants can exist in multiple instances or be replicated across different locations. Examples : A book\u2019s content (as opposed to a single physical copy of the book) A software program (which can be installed on multiple computers) A musical composition (which can be played on different instruments) A dataset and data items Entities with information content","title":"Generically Dependent Continuants (GDC)"},{"location":"ontology/#specifically-dependent-continuants-sdc","text":"Specifically dependent continuants are qualities, roles, or dispositions that exist only in relation to a particular independent continuant . They cannot exist independently and must always be inherent in something else . Qualities - Intrinsic properties of an independent continuant. They describe how an entity is at any moment in time. Examples : The color of a leaf, the weight of a person, the temperature of a liquid. Roles - Situational properties that an entity has based on context or social convention . Examples : The role of a teacher, he status of a patient in a hospital, the role of a machine undergoing maintenance. Dispositions and functions - Potential behaviors or tendencies that an entity has, even if they are not currently being realized. Functions are dispositions that represent the particular purpose of something. Examples : The fragility of glass (it might break if dropped), the solubility of salt (it dissolves in water), a person\u2019s ability to speak multiple languages, the function of an oven to heat something up, the function of a screwdriver to turn screws in and out.","title":"Specifically Dependent Continuants (SDC)"},{"location":"ontology/#occurrents-perdurants","text":"An occurrents is an entity that unfolds itself in time or it is the start or end of such an entity.","title":"Occurrents (Perdurants)"},{"location":"ontology/#processes","text":"Processes are dynamic activities with temporal duration. Examples : A running event, a chemical reaction, cell division.","title":"Processes"},{"location":"ontology/#temporal-regions","text":"These represent divisions of time. Examples : A second, an hour, a historical period.","title":"Temporal Regions"},{"location":"ontology/#spatiotemporal-regions","text":"These combine space and time into a single entity. Examples : The path of a moving object, the trajectory of a planet.","title":"Spatiotemporal Regions"},{"location":"ontology/#relations-in-bfo","text":"BFO defines formal relationships between entities to maintain consistency. Some key relations include: continunat part of \u2013 Indicates compositional relationships. ( Example: A wheel is part of a car. ) occurent part of - Some process has another process as part. ( Example: A conference event has multiple workshop events. ) located in \u2013 Specifies spatial containment. ( Example: A book is located_in a library. ) bearer of \u2013 Assigns specifically dependent continuants to independent continuants. ( Example: A teacher is the bearer of the educator role. ) has participant - Assigns continuants to processes. ( Example: A student participates a lecture event. ) More information about BFO can be found at the GitHub repo and the documentation page .","title":"Relations in BFO"},{"location":"patterns/","text":"Usage Patterns In this section usage patterns and A-Box examples are introduced. NFDI Resources Within NFDIcore, resources are continuants which encompass a wide range of digital creative works, including datasets, collections, and metadata, as well as offered products and services such as data portals, data curation, and data digitization. stateDiagram direction BT #style definitions classDef clazz fill:lightgrey,color:white classDef individual font-size:small #styling of classes class bfo_occurrent clazz class bfo_entity clazz class bfo_continuant clazz #relations bfo_occurrent --> bfo_entity: subClassOf # bfo_continuant --> bfo_entity: subClassOf # bfo_process --> bfo_occurrent: subClassOf # bfo_temporal_region --> bfo_occurrent: subClassOf # bfo_one_dimensional --> bfo_temporal_region: subClassOf # bfo_two_dimensional --> bfo_temporal_region: subClassOf var mermaidConfig = { startOnLoad: false, theme: \"base\", fontFamily: \"Inter, system-ui, sans-serif\", themeVariables: { background: \"#f3f7f8\", primaryColor: \"#e6eef2\", primaryBorderColor: \"#134f5c\", primaryTextColor: \"#0b1320\", lineColor: \"#134f5c\", tertiaryColor: \"#ec941c\", edgeLabelBackground: \"#ffffff\", fontSize: \"14px\" } }; // optional, but harmless window.mermaidConfig = mermaidConfig; stateDiagram direction BT classDef clazz fill:lightgrey,color:white; state \"cto:CTO_0001005 (source item)\" as CTO_SourceItem class CTO_SourceItem clazz state \"nfdicore:NFDI_0000003 (organization)\" as NFDI_organization class NFDI_organization clazz state \"schema:DataFeed\" as SCHEMA_DataFeed class SCHEMA_DataFeed clazz state \"cto:CTO_0001024 (incipit)\" as CTO_Incipit class CTO_Incipit clazz state \"RISM Online\" as org_rism state \"E5313\" as feed_e5313 state \"RISM Resource 201001959\" as si_201001959 state \"201001959 incipit 1.1.1\" as inc_1_1_1 state \"A''C/'B''D'BA/2G4BA/G-B''D/G4-2-/\" as PATTERN_CLASS class PATTERN_CLASS state \"C/\" as TIMESIG class TIMESIG org_rism --> NFDI_organization:a si_201001959 --> CTO_SourceItem:a inc_1_1_1 --> CTO_Incipit:a feed_e5313 --> SCHEMA_DataFeed:a si_201001959 --> inc_1_1_1:CTO_has incipit si_201001959 --> org_rism: NFDI_publishedBy si_201001959 --> feed_e5313:CTO_isReferencedIn inc_1_1_1 --> PATTERN_CLASS:CTO_hasPattern inc_1_1_1 --> key_G:CTO_hasKey inc_1_1_1 --> clef_C1:CTO_hasClef inc_1_1_1 --> keysig_xF:CTO_hasKeySignature inc_1_1_1 --> TIMESIG:CTO_hasIncipitTimeSignature class RISM_201001959 class RISM_inc_1_1_1 class RISM-online class E5313 class G class C1 state \"xF\" as keysig_xF NFDI resources can be associated with academic disciplines (nfdicore:academic discipline, NFDI_0000100) allowing for a more nuanced understanding of their context and relevance. Agents Likewise, in NFDIcore, the concept of Agents extends to both organizations and persons, serving as independent continuants within the BFO ontology. Agents are essential for management of research data resources, and include entities such as research institutions, consortia, universities, companies, and individual researchers or data scientists. Within NFDIcore, agents can be directly linked to resources via different relations; may participate in processes (BFO_0000015) and be associated with diverse roles (BFO_0000023), e.g. nfdicore:publisher role (NFDI_0000193), nfdicore:contributor role (NFDI_0000118), etc. within the processes. Moreover, agents engage in diverse relationships with other agents, e.g. nfdicore:parent organization (NFDI_0000182). Resources within NFDIcore engage in diverse relationships with agents, facilitated by various properties such as nfdicore:contributor, nfdicore:creator, nfdicore:publisher, and nfdicore:contactPoint. While these properties enable direct connections between agents and resources, they may lack some expressivity. E.g. additional information about the relationship like a temporal context, ordering (e.g. for authors). In scenarios requiring more detailed semantics to describe the relationship between an agent and a resource, the modeling proposed by BFO 2020 with processes and roles can be employed. Processes A process (BFO_0000015) can encompass subordinate processes and partial processes. Within NFDIcore, a process often serves to establish connections between agents and information content entities (IAO_0000030), defining the roles (BFO_0000023) of agents with respect to the involved information content entities. Events and Time The bfo:temporal region (BFO_0000008) is an occurrent and is used to represent the structure of time. Thereby, the bfo:zero-dimensional temporal region (BFO_0000148) represents a single instant in time, a precise point in time without any duration, e.g. the date when the NFDI Verein was founded (nfdicore:act of foundation, NFDI_0001020). In contrast, the one-dimensional temporal region (BFO_0000038) refers to a time interval which has a duration, can be finite as well as infinite and events can occur within it. The nfdicore:event (NFDI_0000131) is associated with processes that represent an organized happening that unfolds in time. The nfdicore:event, classified as an occurrent (BFO_0000003), serves as a representation for various happenings such as conferences, and workshops.","title":"Patterns"},{"location":"patterns/#usage-patterns","text":"In this section usage patterns and A-Box examples are introduced.","title":"Usage Patterns"},{"location":"patterns/#nfdi-resources","text":"Within NFDIcore, resources are continuants which encompass a wide range of digital creative works, including datasets, collections, and metadata, as well as offered products and services such as data portals, data curation, and data digitization. stateDiagram direction BT #style definitions classDef clazz fill:lightgrey,color:white classDef individual font-size:small #styling of classes class bfo_occurrent clazz class bfo_entity clazz class bfo_continuant clazz #relations bfo_occurrent --> bfo_entity: subClassOf # bfo_continuant --> bfo_entity: subClassOf # bfo_process --> bfo_occurrent: subClassOf # bfo_temporal_region --> bfo_occurrent: subClassOf # bfo_one_dimensional --> bfo_temporal_region: subClassOf # bfo_two_dimensional --> bfo_temporal_region: subClassOf var mermaidConfig = { startOnLoad: false, theme: \"base\", fontFamily: \"Inter, system-ui, sans-serif\", themeVariables: { background: \"#f3f7f8\", primaryColor: \"#e6eef2\", primaryBorderColor: \"#134f5c\", primaryTextColor: \"#0b1320\", lineColor: \"#134f5c\", tertiaryColor: \"#ec941c\", edgeLabelBackground: \"#ffffff\", fontSize: \"14px\" } }; // optional, but harmless window.mermaidConfig = mermaidConfig; stateDiagram direction BT classDef clazz fill:lightgrey,color:white; state \"cto:CTO_0001005 (source item)\" as CTO_SourceItem class CTO_SourceItem clazz state \"nfdicore:NFDI_0000003 (organization)\" as NFDI_organization class NFDI_organization clazz state \"schema:DataFeed\" as SCHEMA_DataFeed class SCHEMA_DataFeed clazz state \"cto:CTO_0001024 (incipit)\" as CTO_Incipit class CTO_Incipit clazz state \"RISM Online\" as org_rism state \"E5313\" as feed_e5313 state \"RISM Resource 201001959\" as si_201001959 state \"201001959 incipit 1.1.1\" as inc_1_1_1 state \"A''C/'B''D'BA/2G4BA/G-B''D/G4-2-/\" as PATTERN_CLASS class PATTERN_CLASS state \"C/\" as TIMESIG class TIMESIG org_rism --> NFDI_organization:a si_201001959 --> CTO_SourceItem:a inc_1_1_1 --> CTO_Incipit:a feed_e5313 --> SCHEMA_DataFeed:a si_201001959 --> inc_1_1_1:CTO_has incipit si_201001959 --> org_rism: NFDI_publishedBy si_201001959 --> feed_e5313:CTO_isReferencedIn inc_1_1_1 --> PATTERN_CLASS:CTO_hasPattern inc_1_1_1 --> key_G:CTO_hasKey inc_1_1_1 --> clef_C1:CTO_hasClef inc_1_1_1 --> keysig_xF:CTO_hasKeySignature inc_1_1_1 --> TIMESIG:CTO_hasIncipitTimeSignature class RISM_201001959 class RISM_inc_1_1_1 class RISM-online class E5313 class G class C1 state \"xF\" as keysig_xF NFDI resources can be associated with academic disciplines (nfdicore:academic discipline, NFDI_0000100) allowing for a more nuanced understanding of their context and relevance.","title":"NFDI Resources"},{"location":"patterns/#agents","text":"Likewise, in NFDIcore, the concept of Agents extends to both organizations and persons, serving as independent continuants within the BFO ontology. Agents are essential for management of research data resources, and include entities such as research institutions, consortia, universities, companies, and individual researchers or data scientists. Within NFDIcore, agents can be directly linked to resources via different relations; may participate in processes (BFO_0000015) and be associated with diverse roles (BFO_0000023), e.g. nfdicore:publisher role (NFDI_0000193), nfdicore:contributor role (NFDI_0000118), etc. within the processes. Moreover, agents engage in diverse relationships with other agents, e.g. nfdicore:parent organization (NFDI_0000182). Resources within NFDIcore engage in diverse relationships with agents, facilitated by various properties such as nfdicore:contributor, nfdicore:creator, nfdicore:publisher, and nfdicore:contactPoint. While these properties enable direct connections between agents and resources, they may lack some expressivity. E.g. additional information about the relationship like a temporal context, ordering (e.g. for authors). In scenarios requiring more detailed semantics to describe the relationship between an agent and a resource, the modeling proposed by BFO 2020 with processes and roles can be employed.","title":"Agents"},{"location":"patterns/#processes","text":"A process (BFO_0000015) can encompass subordinate processes and partial processes. Within NFDIcore, a process often serves to establish connections between agents and information content entities (IAO_0000030), defining the roles (BFO_0000023) of agents with respect to the involved information content entities.","title":"Processes"},{"location":"patterns/#events-and-time","text":"The bfo:temporal region (BFO_0000008) is an occurrent and is used to represent the structure of time. Thereby, the bfo:zero-dimensional temporal region (BFO_0000148) represents a single instant in time, a precise point in time without any duration, e.g. the date when the NFDI Verein was founded (nfdicore:act of foundation, NFDI_0001020). In contrast, the one-dimensional temporal region (BFO_0000038) refers to a time interval which has a duration, can be finite as well as infinite and events can occur within it. The nfdicore:event (NFDI_0000131) is associated with processes that represent an organized happening that unfolds in time. The nfdicore:event, classified as an occurrent (BFO_0000003), serves as a representation for various happenings such as conferences, and workshops.","title":"Events and Time"},{"location":"questions-2.0/","text":"NFDIcore Competency Questions (CQs) and SPARQL Queries For the design and evaluation different projects contributed competecy questions the ontology should be able to answer. NFDI-MatWerk All NFDI-MatWerk Use Cases are available at the MatWerk Consortium Webpage Use Case 1 CQ: What are the specific services (in digitalization) that are used in a specific academic discipline? (data science) (need to be transferred to the new generation of scientists and professionals) <!-- SPARQL: SELECT ?service ?serviceType WHERE { ?service rdf : type ?serviceType ; nfdicore : subjectArea ex : MaterialScience . ?serviceType rdfs : subClassOf nfdicore : Service . } ``` --> - ** CQ : ** What are the resources and events related to specific processes ( demonstration and teaching ) ? <!--- ** SPARQL : ** ```sparql SELECT ?resource ?event ?process WHERE { { ?resource bfo : RO_0000056 ?process } UNION { ?event bfo : BFO_0000050 ?process . } FILTER ( ?process = ex : Demonstration || ?p = ex : Teaching ) } ``` --> - ** CQ : ** What process are organisations involved in ? and what are their roles in the processes? ( PP participant universities ) - ** SPARQL : ** ```sparql SELECT ?organisation ?role ?process WHERE { ?organisation rdf : type nfdicore : Organization ; bfo : RO_0000087 ?role ; bfo : RO_0000056 ?process . ?role bfo : BFO_0000054 ?process . } Use Case 2 CQ: What metadata schemas and ontologies are utilized in datasets that describe collections ? (materials used in Ni-based superalloys) SPARQL: SELECT ?dataset ?standard WHERE { ?dataset rdf : type nfdicore : Dataset ; nfdicore : representedCollection ?collection ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology . ?ontology rdf : type nfdicore : Ontology .} CQ: What are the standard processes material data sets involved in? (the definition, identification and curation) SPARQL: SELECT ?dataset ?processType WHERE { ?dataset rdf : type nfdicore : Dataset ; nfdicore : subjectArea ex : MaterialScience ; bfo : RO_0000056 ?process . ?process rdf : type ?processType . } CQ: How and which resources are data sets be connected to? ( detailed meta-data and context concerning materials history, data collection ) SPARQL: SELECT ?dataset ?property ?resource WHERE { ?dataset rdf : type nfdicore : Dataset ; ?property ?resource . ?property rdfs : subPropertyOf bfo : IAO_0000136 . } Use Case 03 CQ: What specific types of resources exist within a particular subject area, and what technological methods are employed in their storage?(Tomographic data from different techniques) SPARQL: SELECT ?resource ?resourceType ?technologicalMeans WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : MaterialScience ; nfdicore : technology ?technologicalMeans . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What ontologies are used for resources in a specific subject area? SPARQL: SELECT ?resource ?resourceType ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : MaterialScience ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology . ?ontology rdf : type nfdicore : Ontology . ?resourceType rdfs : subClassOf * nfdicore : Resource .} CQ: What services are there for linkage and enrichment (data integration and access)? What software are they using? SPARQL: SELECT ?service ?software WHERE { ?service rdf : type nfdicore : Linkage\\&Enrichment ; nfdicore : software ?software .} Use Case 04 CQ: What standards and specifications are used for data in a specific subject area? SPARQL: SELECT ?resource ?standards WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard | nfdicore : specification ?standards ; nfdicore : subjectArea ex : MaterialScience . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 05 CQ: What tools and services are used for storing and evaluation of a specific data type? SPARQL: No SPARQL query provided CQ: What standards and specifications are used in a certain process? (data analysis) SPARQL: SELECT ?resource WHERE { VALUES ?type { nfdicore : Standard nfdicore : Specification } ?resource bfo : RO_0000056 ?process ; rdf : type ?type . } Use Case 06 CQ: What ontologies are used in a specific subject area? SPARQL: SELECT ?resource WHERE { ?resource nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ex : MaterialScience . ?ontology rdf : type nfdicore : Ontology . } Use Case 08 CQ: What standards are used for resources in a specific process (standardization)? SPARQL: SELECT ?resource WHERE { ?resource bfo : RO_0000056 ?process ; rdf : type ?type nfdicore : Standard . } CQ: What guidelines are there for describing specific processes? (data analysis/visualization) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : DataAnalysis ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . } Use Case 09 CQ: What data portals, services and software are contributed to a specific consortia? What are the subject area of these resources? SPARQL: SELECT ?resource ?acDisc WHERE { VALUES ?type { nfdicore : Software nfdicore : DataPortal nfdicore : Service } ?resource rdf : type ?type ; nfdicore : subjectArea ?acDisc ; bfo : RO_0000056 ?process . ex : NFDIMatWerk bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . } Use Case 10 CQ: What is the funding organisation of a specific project? SPARQL: SELECT ?funder WHERE { ?funder bfo : RO_0000056 ex : NFDI4Culture ; bfo : RO_0000087 ex : funderRole . ?role bfo : BFO_0000054 ex : NFDI4Culture . } Use Case 12 CQ: What ontologies are used in resources contributed to specific consortia? What is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ex : NFDIMatWerk bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . ?resource bfo : RO_0000056 ?process ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . } Use Case 13 CQ: What specific services (expert knowledge) are there and who is the contact point of these services? SPARQL: SELECT ?service ?contactPoint WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : contactPoint ?contactPoint . } Use Case 15 CQ: Who is the contact point of a specific service? (ontology development) SPARQL: SELECT ?service ?contactPoint WHERE { ?service rdf : type nfdicore : Linkage &Enrichment ; nfdicore : contactPoint ?contactPoint . } Use Case 16 CQ: What ontologies are used in resources and what is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . } Use Case 17 CQ: What ontologies are used in resources and what is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . } NFDI4DataScience All NFDI4DataScience Use Cases (Personas) are available at the NFDI4DataScience Consortium Webpage Use Case Alex CQ: What data participates in a specific task (event)? (data cleaning task) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What services participate in a specific task (event)? ( (data cleaning services) SPARQL: SELECT ?service ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type nfdicore : Service . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . } Use Case Ben CQ: What documenting resources are related to a specific event (task)? Who is a contact point of the documentation process? SPARQL: SELECT ?resource ?agent WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?event ; bfo : OBI_0000299 ?resource . ?resource rdf : type nfdicore : Publication . ?role rdf : type nfdicore : ContactPointRole ; bfo : BFO_0000054 ?process . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . } Use Case Cassie CQ: What (training) services and events are available for specific data task (event)? SPARQL: SELECT distinct ?r ?type ?task WHERE { { ?r bfo : RO_0000056 ?task ; rdf : type nfdicore : Learning\\&Teaching ;} UNION { ?r bfo : BFO_0000050 ?task ; rdf : type nfdicore : Event } ?r rdf : type ?type } Use Case David CQ: Who is a contact point of specific data (experimental)? What is their expertise? SPARQL: SELECT ?resource ?resourceType ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case Emma CQ: List the events in the community, their description and their dates. SPARQL: SELECT ?event ?description ?start ?end WHERE { ?event rdf : type nfdicore : Event ; dc : description ?description ; nfdicore : startDate ?start ; nfdicore : endDate ?end . } Use Case Finn CQ: What are the (legal) guidelines for a specific process? (store sensitive data) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : DataAnalysis ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . } Use Case Gina CQ: What are the relevant publications for a specific subject area or event (task)? SPARQL: SELECT ?resource ?event ?ac_disc WHERE { ?resource rdf : type nfdicore : Publication . { ?resource nfdicore : subjectArea ?ac_disc .} UNION { ?resource bfo : RO_0000056 ?event . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event .} } Use Case Hassan CQ: What services are used in specific projects, which technological means and software are used in the services? If available, list guidelines/publications about these services SPARQL: SELECT ?service ?project ?publication ?tech WHERE { ?service rdf : type ?serviceType ; nfdicore : software | nfdicore : technology ?tech ; bfo : RO_0000056 ?project . OPTIONAL { ?publication rdf : type nfdicore : Publication ; bfo : IAO_0000136 ?service . } ?project rdf : type nfdicore : Project . ?serviceType rdfs : subClassOf * nfdicore : Service . } NFDI4Culture All NFDI4Culture Use Cases are available at the NFDI4Culture Consortium Webpage Use Case 1 CQ: What guidelines are available for a process (storing data) in a specific academic discipline (on musical performances)? SPARQL: SELECT ?resource ?acDis WHERE { ?documenting rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?process ; bfo : OBI_0000299 ?resource . ?resource rdf : type nfdicore : Publication ; nfdicore : subjectArea ?acDis . } CQ: What are the services and events related to specific processes ( structured filing and the handling of standard data)? SPARQL: SELECT ?resource ?event ?process WHERE { { ?resource bfo : RO_0000056 ?process } UNION { ?event bfo : BFO_0000050 ?process . } ?resource rdf : type nfdicore : Service . } Use Case 2 CQ: What standards are there for specific resource type (musical data)? Who is a contact point of the standards? SPARQL: SELECT ?resourceType ?standard ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard ?standard . ?standard nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What services are available for specific data task (event)? (keep different versions accessible) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Service . } Use Case 3 CQ: What best practices are used in resources in a specific academic discipline? SPARQL: SELECT ?bestpractice ?acDisc WHERE { ?resource rdf : type ?resourceType ; nfdicore : bestPractice ?bestpractice ; nfdicore : subjectArea ?acDisc . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What are the best practices to publish a resource using in a certain academic discipline? SPARQL: SELECT ?bestPractice ?acDisc WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDisc . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } CQ: What (training) events are available for specific ask (adequate handling of research data)? What is their timeframe? SPARQL: SELECT ?task ?event ?start ?end WHERE { ?event rdf : type nfdicore : Event ; nfdicore : startDate ?start ; nfdicore : endDate ?end ; bfo : BFO_0000050 ?task . } Use Case 4 CQ: What information about resource is used during the publication process? SPARQL: SELECT ?context ?domain ?range WHERE { ?publProcess rdf : type bfo : IAO_0000444 . { ?publProcess ?context ?object } UNION { ?subject ?context ?publProcess } OPTIONAL { ?context rdfs : domain ?domain ; rdfs : range ?range .} } Use Case 5 CQ: What specific services are available for a certain task? (creating digital inventories of art collections) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Service . } CQ: What licenses are used in the consortia for datasets with images? SPARQL: SELECT ?license ?mediaType WHERE { ?resource rdf : type nfdicore : Dataset ; nfdicore : mediaType ?mediaType ; nfdicore : license ?license . } CQ: Who is a contact point of a specific service? SPARQL: SELECT ?resource ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Service . } CQ: What training events are available for a certain task (publishing and maintaining image data) SPARQL: SELECT ?event ?task WHERE { ?event bfo : BFO_0000050 ?task ; rdf : type nfdicore : Event } Use Case 6 CQ: What are the best practices to publish a software in a certain academic discipline (digital humanities)? SPARQL: SELECT ?bestPractice ?acDis WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDis ; rdf : type nfdicore : Software . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } Use Case 7 CQ: What media types are available for resources in a certain academic discipline (music ethnology) and what are their licenses? SPARQL: SELECT ?academicDisc ?mediaType ?license WHERE { ?resource nfdicore : mediaType ?mediaType ; nfdicore : subjectArea ?academicDisc ; nfdicore : license ?license . } CQ: What teaching and learning services are available in a certain academic discipline (music ethnology)? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . } CQ: What guidelines are available (for an organization) to complete a process (publishing research data) for data in a specific academic discipline (performing arts)? SPARQL: SELECT ?guideline ?resource ?acDis WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?dataPublProcess ; bfo : OBI_0000293 ?resource ; bfo : OBI_0000299 ?guideline . ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis . ?guideline rdf : type nfdicore : Publication . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 8 CQ: What processes (data publication, quality assurance, analyses) are certain resource types (e.g. questionnairs) in a certain academic discipline (e.g. musicology) involved in? SPARQL: SELECT distinct ?process ?resourceType ?acDis WHERE { ?process rdf : type ?processType . ?processType rdfs : subClassOf * bfo : OBI_0000011 . { ?process bfo : OBI_0000293 | bfo : OBI_0000299 | bfo : RO_0000057 ?resource .} UNION { ?resource bfo : RO_0000056 ?process } ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 9 CQ: What are the best practices for publishing research data (resources)? SPARQL: SELECT ?bestPractice WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } Use Case 10 CQ: What organizations and persons are participating in processes related to creative works? SPARQL: SELECT ?agent ?process WHERE { ?agent rdf : type ?agentType ; bfo : RO_0000056 ?process . ?resource bfo : RO_0000056 ?process ; rdf : type ?type . ?type rdfs : subClassOf * nfdicore : CreativeWork . ?agentType rdfs : subClassOf * nfdicore : Agent . } Use Case 11 CQ: What resources that belong to a certain academic discipline (e.g. Art History) are available, what controlled vocabularies are associated with them? SPARQL: SELECT ?resource ?acDis ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . ?resourceType rdfs : subClassOf * nfdicore : Resource .} Use Case 12 CQ: What are the services and events related to specific processes (analysis, structured filing and the handling of standard data)? SPARQL: SELECT distinct ?r ?type ?process WHERE { { ?r bfo : RO_0000056 ?process ; rdf : type nfdicore : Learning\\&Teaching ;} UNION { ?r bfo : BFO_0000050 ?process ; rdf : type nfdicore : Event } ?r rdf : type ?type } Use Case 14 CQ: What resources that belong to a certain academic discipline (e.g. Musicology) are available, what media types do they have and what qualities (e.g. semantic expressivity) are associated with them? SPARQL: SELECT ?resource ?acDis ?mediaType ?quality WHERE { ?resource rdf : type ?resourcetype ; nfdicore : subjectArea ?acDis ; nfdicore : mediaType ?mediaType ; ?p ?quality . ?p rdfs : subPropertyOf * bfo : RO_0000086 . ?resourcetype rdfs : subClassOf * nfdicore : Resource . } Use Case 15 CQ: What resources that belong to a certain academic discipline (e.g. Archaeology) are available and respresented by means of a certain semantic expressivity? SPARQL: SELECT distinct ?resource ?acDis ?semExpressivityType WHERE { ?resource rdf : type ?resourcetype ; nfdicore : subjectArea ?acDis ; nfdicore : semanticExpressivity ?semExpressivity . ?semExpressivity rdf : type ?semExpressivityType . } CQ: What services for teaching and learning are available in a certain academic discipline? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . } Use Case 16 CQ: What services for linking and enriching as well as publishing and disseminating research data are available in a certain academic discipline (eg. Architecture)? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type ?serviceType ; nfdicore : subjectArea ?acDis . VALUES ?serviceType { nfdicore : Linkage\\&Enrichment nfdicore : Publication\\&Dissemination } } CQ: What collections are available in a certain geographical region (e.g. Germany)? SPARQL: SELECT ?collection ?place WHERE { ?collection rdf : type nfdicore : Collection ; nfdicore : location ?place . } Use Case 17 CQ: What services are available for linkage and enrichment as well as storage and preservation of research data? SPARQL: SELECT ?service ?serviceType WHERE { ?service rdf : type ?serviceType ; VALUES ?serviceType { nfdicore : Linkage\\&Enrichment nfdicore : Storage\\&Preservation } } Use Case 18 CQ: Who has certain areas of expertise (computer science) and is a contact of a certain service (training and learning)? SPARQL: SELECT ?agent ?expertise ?service WHERE { ?agent rdf : type ?agentType . ?agentType rdfs : subClassOf * nfdicore : Agent . OPTIONAL { ?agent nfdicore : member ?organisation .} ?agent nfdicore : subjectArea ?expertise . ?service nfdicore : contactPoint ?agent ; rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service .} Use Case 20 CQ: What resource types are available in certain geographical locations? SPARQL: SELECT ?resourceType ?location WHERE { ?resource rdf : type ?resourceType ; nfdicore : location ?location . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 21 CQ: What are the specifications and best practices utilized in the publishing process of a resource in a certain academic discipline (e.g. media science)? SPARQL: SELECT ?bestPractice ?acDisc WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDisc . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } Use Case 22 CQ: What software or services are available for a certain task (event)? (versioning) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?type . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event . { ?type rdfs : subClassOf * nfdicore : Service } UNION { ?type rdfs : subClassOf * nfdicore : Software } } Use Case 23 CQ: Which software is available in a certain academic discipline (music)? SPARQL: SELECT ?academicDisc ?resource WHERE { ?resource rdf : type ?type ; nfdicore : subjectArea ?academicDisc . ?type rdfs : subClassOf * nfdicore : Software . } CQ: What learning and teaching services are available for resources of a certain discipline? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . } Use Case 24 CQ: Which resources are related to a specific subject area (ex:Architecture)? SPARQL: SELECT ?academicDisc ?resource WHERE { ?resource rdf : type ?type ; nfdicore : subjectArea ?academicDisc . ?type rdfs : subClassOf * nfdicore : Resource . } Use Case 25 CQ: Which resources are related to an event that occured in a specific time frame and which mediatype and, if available, semantic expressivity are they available in? SPARQL: SELECT distinct ?resource ?event ?mediaType ?semExpType WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?type ; nfdicore : mediaType ?mediaType . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event . ?type rdfs : subClassOf * nfdicore : Resource . OPTIONAL { ?resource nfdicore : semanticExpressivity ?semExp . ?semExp rdf : type ?semExpType . } } Use Case 26 CQ: Which software was provided by an agent that participates in a project? SPARQL: SELECT ?resource ?project ?agent WHERE { ?resource rdf : type ?type ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?type rdfs : subClassOf * nfdicore : Software . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . } CQ: Which resources and resource types were contributed by an agent that participates in a project? SPARQL: SELECT ?resource ?type ?project ?agent WHERE { ?resource rdf : type ?type ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?type rdfs : subClassOf * nfdicore : Resource . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . } CQ: What persons participate in a project and belong to a certain academic discipline (computer science or digital humanities)? SPARQL: SELECT ?agent ?project ?acDis WHERE { { ?agent bfo : RO_0000056 ?process ; bfo : BFO_0000050 ?project . } UNION { ?agent bfo : RO_0000056 ?project .} ?project rdf : type nfdicore : Project . ?agent nfdicore : subjectArea ?acDis ; rdf : type ?agentType . ?agentType rdfs : subClassOf * nfdicore : Agent . } CQ: What are lerning and training services are contributed by an agent that participates in a project? SPARQL: SELECT ?resource ?project ?agent WHERE { ?resource rdf : type nfdicore : Learning\\&Teaching ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . } Use Case 27 CQ: What specifications are there for certain types of resources and mediatypes of resources? SPARQL: SELECT ?resourcetype ?specification ?mediatype WHERE { ?resource rdf : type ?resourcetype . ?resourcetype rdfs : subClassOf * nfdicore : Resource . OPTIONAL { ?resource nfdicore : specification ?specification . } OPTIONAL { ?resource nfdicore : mediaType ?mediatype . } } Use Case 28 CQ: What controlled vocabularies are available for certain resource types? SPARQL: SELECT ?resourceType ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . ?resourceType rdfs : subClassOf * nfdicore : Resource .} NFDI4Memory All NFDI4Memory Use Cases are available at the NFDI4Memory Consortium Webpage Use Case 1 CQ: Who has certain areas of expertise (e.g. IT) and is involved in a certain service (training and education)? SPARQL: SELECT ?contactPoint WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : contactPoint ?contactPoint . ?contactPoint rdf : type nfdicore : Person ; nfdicore : subjectArea ex : IT . } Use Case 3 CQ: What are the best practices for a certain resource? SPARQL: SELECT ?bestpractice WHERE { ?resource rdf : type ?resourceType ; nfdicore : bestPractice ?bestpractice . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 5 CQ: What software is frequently used and who are their contact points? SPARQL: SELECT ?software ( count ( ?software ) as ?count ) ?contactPoint WHERE { ?software nfdicore : contactPoint ?contactPoint . ?resource nfdicore : software ?software . } GROUP BY ?software ?contactPoint ORDER BY DESC ( ?count ) Use Case 6 CQ: What are the controlled vocabularies used in the consortium? SPARQL: SELECT ?standard WHERE { ex : NFDI4Memory bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . ?resource bfo : RO_0000056 ?process ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?controlledVoc . ?controlledVoc rdf : type nfdicore : ControlledVocabulary . } CQ: What tools and services for a certain purpose are used in the consortium? SPARQL: SELECT ?service ?task WHERE { ?service rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service ; rdfs : label ?task . } Use Case 7 CQ: Which licences are used in certain data resources types? SPARQL: SELECT ?license ( count ( distinct ?license ) as ?count ) ?resourceType WHERE { ?resource rdf : type ?resourceType ; nfdicore : license ?license . ?resourceType rdfs : subClassOf * nfdicore : Resource . } GROUP BY ?license ?resourceType Use Case 8 CQ: What are the guidlines for a specific task? (publishing research data) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : ResearchDataPublication ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . } Use Case 9 CQ: Which organizations (archives) located in Germany contribute certain resources and what licences do they use? SPARQL: SELECT DISTINCT ?organization ?resource ?license WHERE { ?organization rdf : type nfdicore : Organization ; nfdicore : location ex : Germany ; bfo : RO_0000087 ?role ; bfo : RO_0000056 ?process . ?resource rdf : type nfdicore : Resource ; bfo : RO_0000056 ?process ; nfdicore : license ?license . } CQ: What guidelines for data provenance for certain data type (archival) exist? SPARQL: SELECT ?guideline ?resourceType WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?dataprovenance ; bfo : OBI_0000293 ?resource ; bfo : OBI_0000299 ?guideline . ?guideline rdf : type nfdicore : Publication . ?resource rdf : type ?resourceType . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 11 CQ: Which organization has members with expertise in computer science or information technology? SPARQL: SELECT ?organization ?member WHERE { ?organization rdf : type nfdicore : Organisation . ?person rdf : type nfdicore : Person ; nfdicore : member ?organization . { ?person nfdicore : subjectArea ex : ComputerScience . } UNION { ?person nfdicore : subjectArea ex : InformationTechnology . } } Use Case 13 CQ: What data formats are used for resources in a certain academic discipline? SPARQL: SELECT ?academicDisc ?mediaType WHERE { ?resource nfdicore : mediaType ?mediaType ; nfdicore : subjectArea ?academicDisc . } Use Case 14 CQ: What licences given for resources in a certain media type? SPARQL: SELECT ?license ?mediaType WHERE { ?resource rdf : type ?resourceType ; nfdicore : mediaType ?mediaType ; nfdicore : license ?license . ?resourceType rdfs : subClassOf * nfdicore : Resource . } Use Case 15 CQ: Which services are available for a certain task? SPARQL: SELECT ?service ?task WHERE { ?service rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service ; rdfs : label ?task . } CQ: Which softwares are available in a certain discipline? SPARQL: SELECT ?resource ?academicDisc WHERE { ?resource rdf : type nfdicore : Software ; nfdicore : subjectArea ?academicDisc . } Use Case 16 CQ: Which organization offers a certain area of expertise? (IT) (including people in the organizations and their expertise) SPARQL: SELECT ?orga WHERE { ?orga rdf : type nfdicore : Organization . ?person nfdicore : member ?orga . { ?orga nfdicore : subjectArea ex : IT } UNION { ?person nfdicore : subjectArea ex : IT } } CQ: Which organization is a contact point for a certain service? SPARQL: SELECT distinct ?service ?agent WHERE { ?orga rdf : type nfdicore : Organization . ?service rdf : type ?serviceType . ?service nfdicore : contactPoint ?agent . ?serviceType rdfs : subClassOf * nfdicore : Service . { ?agent rdf : type nfdicore : Organization } UNION { ?person nfdicore : member ?agent } } Use Case 17 CQ: What services in a certain academic discipline (computer science) are there? SPARQL: SELECT ?resource WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : ComputerScience . ?resourceType rdfs : subClassOf * nfdicore : Service . } Use Case 23 CQ: What are the best practices to publish a resource using a certain (open) licence? SPARQL: SELECT ?bestPractice ?license WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : license ?license . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } Use Case 42 CQ: What controlled vocabularies can be utizlized (in the publishing process) of a resource? SPARQL: SELECT ?standard WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . } SPARQL: SELECT ?standard WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?standard bfo : RO_0000056 ?process ; rdf : type ?type ; nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"NFDIcore Competency Questions (CQs) and SPARQL Queries"},{"location":"questions-2.0/#nfdicore-competency-questions-cqs-and-sparql-queries","text":"For the design and evaluation different projects contributed competecy questions the ontology should be able to answer.","title":"NFDIcore Competency Questions (CQs) and SPARQL Queries"},{"location":"questions-2.0/#nfdi-matwerk","text":"All NFDI-MatWerk Use Cases are available at the MatWerk Consortium Webpage","title":"NFDI-MatWerk"},{"location":"questions-2.0/#use-case-1","text":"CQ: What are the specific services (in digitalization) that are used in a specific academic discipline? (data science) (need to be transferred to the new generation of scientists and professionals) <!-- SPARQL: SELECT ?service ?serviceType WHERE { ?service rdf : type ?serviceType ; nfdicore : subjectArea ex : MaterialScience . ?serviceType rdfs : subClassOf nfdicore : Service . } ``` --> - ** CQ : ** What are the resources and events related to specific processes ( demonstration and teaching ) ? <!--- ** SPARQL : ** ```sparql SELECT ?resource ?event ?process WHERE { { ?resource bfo : RO_0000056 ?process } UNION { ?event bfo : BFO_0000050 ?process . } FILTER ( ?process = ex : Demonstration || ?p = ex : Teaching ) } ``` --> - ** CQ : ** What process are organisations involved in ? and what are their roles in the processes? ( PP participant universities ) - ** SPARQL : ** ```sparql SELECT ?organisation ?role ?process WHERE { ?organisation rdf : type nfdicore : Organization ; bfo : RO_0000087 ?role ; bfo : RO_0000056 ?process . ?role bfo : BFO_0000054 ?process . }","title":"Use Case 1"},{"location":"questions-2.0/#use-case-2","text":"CQ: What metadata schemas and ontologies are utilized in datasets that describe collections ? (materials used in Ni-based superalloys) SPARQL: SELECT ?dataset ?standard WHERE { ?dataset rdf : type nfdicore : Dataset ; nfdicore : representedCollection ?collection ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology . ?ontology rdf : type nfdicore : Ontology .} CQ: What are the standard processes material data sets involved in? (the definition, identification and curation) SPARQL: SELECT ?dataset ?processType WHERE { ?dataset rdf : type nfdicore : Dataset ; nfdicore : subjectArea ex : MaterialScience ; bfo : RO_0000056 ?process . ?process rdf : type ?processType . } CQ: How and which resources are data sets be connected to? ( detailed meta-data and context concerning materials history, data collection ) SPARQL: SELECT ?dataset ?property ?resource WHERE { ?dataset rdf : type nfdicore : Dataset ; ?property ?resource . ?property rdfs : subPropertyOf bfo : IAO_0000136 . }","title":"Use Case 2"},{"location":"questions-2.0/#use-case-03","text":"CQ: What specific types of resources exist within a particular subject area, and what technological methods are employed in their storage?(Tomographic data from different techniques) SPARQL: SELECT ?resource ?resourceType ?technologicalMeans WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : MaterialScience ; nfdicore : technology ?technologicalMeans . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What ontologies are used for resources in a specific subject area? SPARQL: SELECT ?resource ?resourceType ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : MaterialScience ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology . ?ontology rdf : type nfdicore : Ontology . ?resourceType rdfs : subClassOf * nfdicore : Resource .} CQ: What services are there for linkage and enrichment (data integration and access)? What software are they using? SPARQL: SELECT ?service ?software WHERE { ?service rdf : type nfdicore : Linkage\\&Enrichment ; nfdicore : software ?software .}","title":"Use Case 03"},{"location":"questions-2.0/#use-case-04","text":"CQ: What standards and specifications are used for data in a specific subject area? SPARQL: SELECT ?resource ?standards WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard | nfdicore : specification ?standards ; nfdicore : subjectArea ex : MaterialScience . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 04"},{"location":"questions-2.0/#use-case-05","text":"CQ: What tools and services are used for storing and evaluation of a specific data type? SPARQL: No SPARQL query provided CQ: What standards and specifications are used in a certain process? (data analysis) SPARQL: SELECT ?resource WHERE { VALUES ?type { nfdicore : Standard nfdicore : Specification } ?resource bfo : RO_0000056 ?process ; rdf : type ?type . }","title":"Use Case 05"},{"location":"questions-2.0/#use-case-06","text":"CQ: What ontologies are used in a specific subject area? SPARQL: SELECT ?resource WHERE { ?resource nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ex : MaterialScience . ?ontology rdf : type nfdicore : Ontology . }","title":"Use Case 06"},{"location":"questions-2.0/#use-case-08","text":"CQ: What standards are used for resources in a specific process (standardization)? SPARQL: SELECT ?resource WHERE { ?resource bfo : RO_0000056 ?process ; rdf : type ?type nfdicore : Standard . } CQ: What guidelines are there for describing specific processes? (data analysis/visualization) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : DataAnalysis ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . }","title":"Use Case 08"},{"location":"questions-2.0/#use-case-09","text":"CQ: What data portals, services and software are contributed to a specific consortia? What are the subject area of these resources? SPARQL: SELECT ?resource ?acDisc WHERE { VALUES ?type { nfdicore : Software nfdicore : DataPortal nfdicore : Service } ?resource rdf : type ?type ; nfdicore : subjectArea ?acDisc ; bfo : RO_0000056 ?process . ex : NFDIMatWerk bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . }","title":"Use Case 09"},{"location":"questions-2.0/#use-case-10","text":"CQ: What is the funding organisation of a specific project? SPARQL: SELECT ?funder WHERE { ?funder bfo : RO_0000056 ex : NFDI4Culture ; bfo : RO_0000087 ex : funderRole . ?role bfo : BFO_0000054 ex : NFDI4Culture . }","title":"Use Case 10"},{"location":"questions-2.0/#use-case-12","text":"CQ: What ontologies are used in resources contributed to specific consortia? What is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ex : NFDIMatWerk bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . ?resource bfo : RO_0000056 ?process ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . }","title":"Use Case 12"},{"location":"questions-2.0/#use-case-13","text":"CQ: What specific services (expert knowledge) are there and who is the contact point of these services? SPARQL: SELECT ?service ?contactPoint WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : contactPoint ?contactPoint . }","title":"Use Case 13"},{"location":"questions-2.0/#use-case-15","text":"CQ: Who is the contact point of a specific service? (ontology development) SPARQL: SELECT ?service ?contactPoint WHERE { ?service rdf : type nfdicore : Linkage &Enrichment ; nfdicore : contactPoint ?contactPoint . }","title":"Use Case 15"},{"location":"questions-2.0/#use-case-16","text":"CQ: What ontologies are used in resources and what is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . }","title":"Use Case 16"},{"location":"questions-2.0/#use-case-17","text":"CQ: What ontologies are used in resources and what is their subject area? SPARQL: SELECT ?standard ?acDisc WHERE { ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?ontology ; nfdicore : subjectArea ?acDisc . ?ontology rdf : type nfdicore : Ontology . }","title":"Use Case 17"},{"location":"questions-2.0/#nfdi4datascience","text":"All NFDI4DataScience Use Cases (Personas) are available at the NFDI4DataScience Consortium Webpage","title":"NFDI4DataScience"},{"location":"questions-2.0/#use-case-alex","text":"CQ: What data participates in a specific task (event)? (data cleaning task) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What services participate in a specific task (event)? ( (data cleaning services) SPARQL: SELECT ?service ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type nfdicore : Service . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . }","title":"Use Case Alex"},{"location":"questions-2.0/#use-case-ben","text":"CQ: What documenting resources are related to a specific event (task)? Who is a contact point of the documentation process? SPARQL: SELECT ?resource ?agent WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?event ; bfo : OBI_0000299 ?resource . ?resource rdf : type nfdicore : Publication . ?role rdf : type nfdicore : ContactPointRole ; bfo : BFO_0000054 ?process . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . }","title":"Use Case Ben"},{"location":"questions-2.0/#use-case-cassie","text":"CQ: What (training) services and events are available for specific data task (event)? SPARQL: SELECT distinct ?r ?type ?task WHERE { { ?r bfo : RO_0000056 ?task ; rdf : type nfdicore : Learning\\&Teaching ;} UNION { ?r bfo : BFO_0000050 ?task ; rdf : type nfdicore : Event } ?r rdf : type ?type }","title":"Use Case Cassie"},{"location":"questions-2.0/#use-case-david","text":"CQ: Who is a contact point of specific data (experimental)? What is their expertise? SPARQL: SELECT ?resource ?resourceType ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case David"},{"location":"questions-2.0/#use-case-emma","text":"CQ: List the events in the community, their description and their dates. SPARQL: SELECT ?event ?description ?start ?end WHERE { ?event rdf : type nfdicore : Event ; dc : description ?description ; nfdicore : startDate ?start ; nfdicore : endDate ?end . }","title":"Use Case Emma"},{"location":"questions-2.0/#use-case-finn","text":"CQ: What are the (legal) guidelines for a specific process? (store sensitive data) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : DataAnalysis ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . }","title":"Use Case Finn"},{"location":"questions-2.0/#use-case-gina","text":"CQ: What are the relevant publications for a specific subject area or event (task)? SPARQL: SELECT ?resource ?event ?ac_disc WHERE { ?resource rdf : type nfdicore : Publication . { ?resource nfdicore : subjectArea ?ac_disc .} UNION { ?resource bfo : RO_0000056 ?event . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event .} }","title":"Use Case Gina"},{"location":"questions-2.0/#use-case-hassan","text":"CQ: What services are used in specific projects, which technological means and software are used in the services? If available, list guidelines/publications about these services SPARQL: SELECT ?service ?project ?publication ?tech WHERE { ?service rdf : type ?serviceType ; nfdicore : software | nfdicore : technology ?tech ; bfo : RO_0000056 ?project . OPTIONAL { ?publication rdf : type nfdicore : Publication ; bfo : IAO_0000136 ?service . } ?project rdf : type nfdicore : Project . ?serviceType rdfs : subClassOf * nfdicore : Service . }","title":"Use Case Hassan"},{"location":"questions-2.0/#nfdi4culture","text":"All NFDI4Culture Use Cases are available at the NFDI4Culture Consortium Webpage","title":"NFDI4Culture"},{"location":"questions-2.0/#use-case-1_1","text":"CQ: What guidelines are available for a process (storing data) in a specific academic discipline (on musical performances)? SPARQL: SELECT ?resource ?acDis WHERE { ?documenting rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?process ; bfo : OBI_0000299 ?resource . ?resource rdf : type nfdicore : Publication ; nfdicore : subjectArea ?acDis . } CQ: What are the services and events related to specific processes ( structured filing and the handling of standard data)? SPARQL: SELECT ?resource ?event ?process WHERE { { ?resource bfo : RO_0000056 ?process } UNION { ?event bfo : BFO_0000050 ?process . } ?resource rdf : type nfdicore : Service . }","title":"Use Case 1"},{"location":"questions-2.0/#use-case-2_1","text":"CQ: What standards are there for specific resource type (musical data)? Who is a contact point of the standards? SPARQL: SELECT ?resourceType ?standard ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard ?standard . ?standard nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What services are available for specific data task (event)? (keep different versions accessible) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Service . }","title":"Use Case 2"},{"location":"questions-2.0/#use-case-3","text":"CQ: What best practices are used in resources in a specific academic discipline? SPARQL: SELECT ?bestpractice ?acDisc WHERE { ?resource rdf : type ?resourceType ; nfdicore : bestPractice ?bestpractice ; nfdicore : subjectArea ?acDisc . ?resourceType rdfs : subClassOf * nfdicore : Resource . } CQ: What are the best practices to publish a resource using in a certain academic discipline? SPARQL: SELECT ?bestPractice ?acDisc WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDisc . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } } CQ: What (training) events are available for specific ask (adequate handling of research data)? What is their timeframe? SPARQL: SELECT ?task ?event ?start ?end WHERE { ?event rdf : type nfdicore : Event ; nfdicore : startDate ?start ; nfdicore : endDate ?end ; bfo : BFO_0000050 ?task . }","title":"Use Case 3"},{"location":"questions-2.0/#use-case-4","text":"CQ: What information about resource is used during the publication process? SPARQL: SELECT ?context ?domain ?range WHERE { ?publProcess rdf : type bfo : IAO_0000444 . { ?publProcess ?context ?object } UNION { ?subject ?context ?publProcess } OPTIONAL { ?context rdfs : domain ?domain ; rdfs : range ?range .} }","title":"Use Case 4"},{"location":"questions-2.0/#use-case-5","text":"CQ: What specific services are available for a certain task? (creating digital inventories of art collections) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?resourceType . ?event rdf : type ?eventType ; ?eventType rdfs : subClassOf * nfdicore : Event . ?resourceType rdfs : subClassOf * nfdicore : Service . } CQ: What licenses are used in the consortia for datasets with images? SPARQL: SELECT ?license ?mediaType WHERE { ?resource rdf : type nfdicore : Dataset ; nfdicore : mediaType ?mediaType ; nfdicore : license ?license . } CQ: Who is a contact point of a specific service? SPARQL: SELECT ?resource ?contactPoint WHERE { ?resource rdf : type ?resourceType ; nfdicore : contactPoint ?contactPoint . ?resourceType rdfs : subClassOf * nfdicore : Service . } CQ: What training events are available for a certain task (publishing and maintaining image data) SPARQL: SELECT ?event ?task WHERE { ?event bfo : BFO_0000050 ?task ; rdf : type nfdicore : Event }","title":"Use Case 5"},{"location":"questions-2.0/#use-case-6","text":"CQ: What are the best practices to publish a software in a certain academic discipline (digital humanities)? SPARQL: SELECT ?bestPractice ?acDis WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDis ; rdf : type nfdicore : Software . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"Use Case 6"},{"location":"questions-2.0/#use-case-7","text":"CQ: What media types are available for resources in a certain academic discipline (music ethnology) and what are their licenses? SPARQL: SELECT ?academicDisc ?mediaType ?license WHERE { ?resource nfdicore : mediaType ?mediaType ; nfdicore : subjectArea ?academicDisc ; nfdicore : license ?license . } CQ: What teaching and learning services are available in a certain academic discipline (music ethnology)? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . } CQ: What guidelines are available (for an organization) to complete a process (publishing research data) for data in a specific academic discipline (performing arts)? SPARQL: SELECT ?guideline ?resource ?acDis WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?dataPublProcess ; bfo : OBI_0000293 ?resource ; bfo : OBI_0000299 ?guideline . ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis . ?guideline rdf : type nfdicore : Publication . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 7"},{"location":"questions-2.0/#use-case-8","text":"CQ: What processes (data publication, quality assurance, analyses) are certain resource types (e.g. questionnairs) in a certain academic discipline (e.g. musicology) involved in? SPARQL: SELECT distinct ?process ?resourceType ?acDis WHERE { ?process rdf : type ?processType . ?processType rdfs : subClassOf * bfo : OBI_0000011 . { ?process bfo : OBI_0000293 | bfo : OBI_0000299 | bfo : RO_0000057 ?resource .} UNION { ?resource bfo : RO_0000056 ?process } ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 8"},{"location":"questions-2.0/#use-case-9","text":"CQ: What are the best practices for publishing research data (resources)? SPARQL: SELECT ?bestPractice WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"Use Case 9"},{"location":"questions-2.0/#use-case-10_1","text":"CQ: What organizations and persons are participating in processes related to creative works? SPARQL: SELECT ?agent ?process WHERE { ?agent rdf : type ?agentType ; bfo : RO_0000056 ?process . ?resource bfo : RO_0000056 ?process ; rdf : type ?type . ?type rdfs : subClassOf * nfdicore : CreativeWork . ?agentType rdfs : subClassOf * nfdicore : Agent . }","title":"Use Case 10"},{"location":"questions-2.0/#use-case-11","text":"CQ: What resources that belong to a certain academic discipline (e.g. Art History) are available, what controlled vocabularies are associated with them? SPARQL: SELECT ?resource ?acDis ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ?acDis ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . ?resourceType rdfs : subClassOf * nfdicore : Resource .}","title":"Use Case 11"},{"location":"questions-2.0/#use-case-12_1","text":"CQ: What are the services and events related to specific processes (analysis, structured filing and the handling of standard data)? SPARQL: SELECT distinct ?r ?type ?process WHERE { { ?r bfo : RO_0000056 ?process ; rdf : type nfdicore : Learning\\&Teaching ;} UNION { ?r bfo : BFO_0000050 ?process ; rdf : type nfdicore : Event } ?r rdf : type ?type }","title":"Use Case 12"},{"location":"questions-2.0/#use-case-14","text":"CQ: What resources that belong to a certain academic discipline (e.g. Musicology) are available, what media types do they have and what qualities (e.g. semantic expressivity) are associated with them? SPARQL: SELECT ?resource ?acDis ?mediaType ?quality WHERE { ?resource rdf : type ?resourcetype ; nfdicore : subjectArea ?acDis ; nfdicore : mediaType ?mediaType ; ?p ?quality . ?p rdfs : subPropertyOf * bfo : RO_0000086 . ?resourcetype rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 14"},{"location":"questions-2.0/#use-case-15_1","text":"CQ: What resources that belong to a certain academic discipline (e.g. Archaeology) are available and respresented by means of a certain semantic expressivity? SPARQL: SELECT distinct ?resource ?acDis ?semExpressivityType WHERE { ?resource rdf : type ?resourcetype ; nfdicore : subjectArea ?acDis ; nfdicore : semanticExpressivity ?semExpressivity . ?semExpressivity rdf : type ?semExpressivityType . } CQ: What services for teaching and learning are available in a certain academic discipline? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . }","title":"Use Case 15"},{"location":"questions-2.0/#use-case-16_1","text":"CQ: What services for linking and enriching as well as publishing and disseminating research data are available in a certain academic discipline (eg. Architecture)? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type ?serviceType ; nfdicore : subjectArea ?acDis . VALUES ?serviceType { nfdicore : Linkage\\&Enrichment nfdicore : Publication\\&Dissemination } } CQ: What collections are available in a certain geographical region (e.g. Germany)? SPARQL: SELECT ?collection ?place WHERE { ?collection rdf : type nfdicore : Collection ; nfdicore : location ?place . }","title":"Use Case 16"},{"location":"questions-2.0/#use-case-17_1","text":"CQ: What services are available for linkage and enrichment as well as storage and preservation of research data? SPARQL: SELECT ?service ?serviceType WHERE { ?service rdf : type ?serviceType ; VALUES ?serviceType { nfdicore : Linkage\\&Enrichment nfdicore : Storage\\&Preservation } }","title":"Use Case 17"},{"location":"questions-2.0/#use-case-18","text":"CQ: Who has certain areas of expertise (computer science) and is a contact of a certain service (training and learning)? SPARQL: SELECT ?agent ?expertise ?service WHERE { ?agent rdf : type ?agentType . ?agentType rdfs : subClassOf * nfdicore : Agent . OPTIONAL { ?agent nfdicore : member ?organisation .} ?agent nfdicore : subjectArea ?expertise . ?service nfdicore : contactPoint ?agent ; rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service .}","title":"Use Case 18"},{"location":"questions-2.0/#use-case-20","text":"CQ: What resource types are available in certain geographical locations? SPARQL: SELECT ?resourceType ?location WHERE { ?resource rdf : type ?resourceType ; nfdicore : location ?location . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 20"},{"location":"questions-2.0/#use-case-21","text":"CQ: What are the specifications and best practices utilized in the publishing process of a resource in a certain academic discipline (e.g. media science)? SPARQL: SELECT ?bestPractice ?acDisc WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : subjectArea ?acDisc . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"Use Case 21"},{"location":"questions-2.0/#use-case-22","text":"CQ: What software or services are available for a certain task (event)? (versioning) SPARQL: SELECT ?resource ?event WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?type . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event . { ?type rdfs : subClassOf * nfdicore : Service } UNION { ?type rdfs : subClassOf * nfdicore : Software } }","title":"Use Case 22"},{"location":"questions-2.0/#use-case-23","text":"CQ: Which software is available in a certain academic discipline (music)? SPARQL: SELECT ?academicDisc ?resource WHERE { ?resource rdf : type ?type ; nfdicore : subjectArea ?academicDisc . ?type rdfs : subClassOf * nfdicore : Software . } CQ: What learning and teaching services are available for resources of a certain discipline? SPARQL: SELECT ?service ?acDis WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : subjectArea ?acDis . }","title":"Use Case 23"},{"location":"questions-2.0/#use-case-24","text":"CQ: Which resources are related to a specific subject area (ex:Architecture)? SPARQL: SELECT ?academicDisc ?resource WHERE { ?resource rdf : type ?type ; nfdicore : subjectArea ?academicDisc . ?type rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 24"},{"location":"questions-2.0/#use-case-25","text":"CQ: Which resources are related to an event that occured in a specific time frame and which mediatype and, if available, semantic expressivity are they available in? SPARQL: SELECT distinct ?resource ?event ?mediaType ?semExpType WHERE { ?resource bfo : RO_0000056 ?event ; rdf : type ?type ; nfdicore : mediaType ?mediaType . ?event rdf : type ?eventType . ?eventType rdfs : subClassOf * nfdicore : Event . ?type rdfs : subClassOf * nfdicore : Resource . OPTIONAL { ?resource nfdicore : semanticExpressivity ?semExp . ?semExp rdf : type ?semExpType . } }","title":"Use Case 25"},{"location":"questions-2.0/#use-case-26","text":"CQ: Which software was provided by an agent that participates in a project? SPARQL: SELECT ?resource ?project ?agent WHERE { ?resource rdf : type ?type ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?type rdfs : subClassOf * nfdicore : Software . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . } CQ: Which resources and resource types were contributed by an agent that participates in a project? SPARQL: SELECT ?resource ?type ?project ?agent WHERE { ?resource rdf : type ?type ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?type rdfs : subClassOf * nfdicore : Resource . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . } CQ: What persons participate in a project and belong to a certain academic discipline (computer science or digital humanities)? SPARQL: SELECT ?agent ?project ?acDis WHERE { { ?agent bfo : RO_0000056 ?process ; bfo : BFO_0000050 ?project . } UNION { ?agent bfo : RO_0000056 ?project .} ?project rdf : type nfdicore : Project . ?agent nfdicore : subjectArea ?acDis ; rdf : type ?agentType . ?agentType rdfs : subClassOf * nfdicore : Agent . } CQ: What are lerning and training services are contributed by an agent that participates in a project? SPARQL: SELECT ?resource ?project ?agent WHERE { ?resource rdf : type nfdicore : Learning\\&Teaching ; bfo : RO_0000056 ?process . ?process rdf : type nfdicore : Contributing ; bfo : BFO_0000050 ?project . ?project rdf : type nfdicore : Project . ?role bfo : BFO_0000054 ?process . ?agent bfo : RO_0000056 ?process ; bfo : RO_0000087 ?role . ?role rdf : type nfdicore : ContributorRole . }","title":"Use Case 26"},{"location":"questions-2.0/#use-case-27","text":"CQ: What specifications are there for certain types of resources and mediatypes of resources? SPARQL: SELECT ?resourcetype ?specification ?mediatype WHERE { ?resource rdf : type ?resourcetype . ?resourcetype rdfs : subClassOf * nfdicore : Resource . OPTIONAL { ?resource nfdicore : specification ?specification . } OPTIONAL { ?resource nfdicore : mediaType ?mediatype . } }","title":"Use Case 27"},{"location":"questions-2.0/#use-case-28","text":"CQ: What controlled vocabularies are available for certain resource types? SPARQL: SELECT ?resourceType ?standard WHERE { ?resource rdf : type ?resourceType ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . ?resourceType rdfs : subClassOf * nfdicore : Resource .}","title":"Use Case 28"},{"location":"questions-2.0/#nfdi4memory","text":"All NFDI4Memory Use Cases are available at the NFDI4Memory Consortium Webpage","title":"NFDI4Memory"},{"location":"questions-2.0/#use-case-1_2","text":"CQ: Who has certain areas of expertise (e.g. IT) and is involved in a certain service (training and education)? SPARQL: SELECT ?contactPoint WHERE { ?service rdf : type nfdicore : Learning\\&Teaching ; nfdicore : contactPoint ?contactPoint . ?contactPoint rdf : type nfdicore : Person ; nfdicore : subjectArea ex : IT . }","title":"Use Case 1"},{"location":"questions-2.0/#use-case-3_1","text":"CQ: What are the best practices for a certain resource? SPARQL: SELECT ?bestpractice WHERE { ?resource rdf : type ?resourceType ; nfdicore : bestPractice ?bestpractice . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 3"},{"location":"questions-2.0/#use-case-5_1","text":"CQ: What software is frequently used and who are their contact points? SPARQL: SELECT ?software ( count ( ?software ) as ?count ) ?contactPoint WHERE { ?software nfdicore : contactPoint ?contactPoint . ?resource nfdicore : software ?software . } GROUP BY ?software ?contactPoint ORDER BY DESC ( ?count )","title":"Use Case 5"},{"location":"questions-2.0/#use-case-6_1","text":"CQ: What are the controlled vocabularies used in the consortium? SPARQL: SELECT ?standard WHERE { ex : NFDI4Memory bfo : RO_0000056 ?process ; bfo : RO_0000087 ex : Repository . ?role bfo : BFO_0000054 ?process . ?process rdf : type nfdicore : Contributing . ?resource bfo : RO_0000056 ?process ; nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?controlledVoc . ?controlledVoc rdf : type nfdicore : ControlledVocabulary . } CQ: What tools and services for a certain purpose are used in the consortium? SPARQL: SELECT ?service ?task WHERE { ?service rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service ; rdfs : label ?task . }","title":"Use Case 6"},{"location":"questions-2.0/#use-case-7_1","text":"CQ: Which licences are used in certain data resources types? SPARQL: SELECT ?license ( count ( distinct ?license ) as ?count ) ?resourceType WHERE { ?resource rdf : type ?resourceType ; nfdicore : license ?license . ?resourceType rdfs : subClassOf * nfdicore : Resource . } GROUP BY ?license ?resourceType","title":"Use Case 7"},{"location":"questions-2.0/#use-case-8_1","text":"CQ: What are the guidlines for a specific task? (publishing research data) SPARQL: SELECT ?resource WHERE { ?process rdf : type bfo : IAO_0000572 ; %documenting bfo : BFO_0000050 ex : ResearchDataPublication ; %partOf bfo : OBI_0000299 ?resource . %has_specified_output ?resource rdf : type nfdicore : Publication . }","title":"Use Case 8"},{"location":"questions-2.0/#use-case-9_1","text":"CQ: Which organizations (archives) located in Germany contribute certain resources and what licences do they use? SPARQL: SELECT DISTINCT ?organization ?resource ?license WHERE { ?organization rdf : type nfdicore : Organization ; nfdicore : location ex : Germany ; bfo : RO_0000087 ?role ; bfo : RO_0000056 ?process . ?resource rdf : type nfdicore : Resource ; bfo : RO_0000056 ?process ; nfdicore : license ?license . } CQ: What guidelines for data provenance for certain data type (archival) exist? SPARQL: SELECT ?guideline ?resourceType WHERE { ?process rdf : type bfo : IAO_0000572 ; bfo : BFO_0000050 ?dataprovenance ; bfo : OBI_0000293 ?resource ; bfo : OBI_0000299 ?guideline . ?guideline rdf : type nfdicore : Publication . ?resource rdf : type ?resourceType . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 9"},{"location":"questions-2.0/#use-case-11_1","text":"CQ: Which organization has members with expertise in computer science or information technology? SPARQL: SELECT ?organization ?member WHERE { ?organization rdf : type nfdicore : Organisation . ?person rdf : type nfdicore : Person ; nfdicore : member ?organization . { ?person nfdicore : subjectArea ex : ComputerScience . } UNION { ?person nfdicore : subjectArea ex : InformationTechnology . } }","title":"Use Case 11"},{"location":"questions-2.0/#use-case-13_1","text":"CQ: What data formats are used for resources in a certain academic discipline? SPARQL: SELECT ?academicDisc ?mediaType WHERE { ?resource nfdicore : mediaType ?mediaType ; nfdicore : subjectArea ?academicDisc . }","title":"Use Case 13"},{"location":"questions-2.0/#use-case-14_1","text":"CQ: What licences given for resources in a certain media type? SPARQL: SELECT ?license ?mediaType WHERE { ?resource rdf : type ?resourceType ; nfdicore : mediaType ?mediaType ; nfdicore : license ?license . ?resourceType rdfs : subClassOf * nfdicore : Resource . }","title":"Use Case 14"},{"location":"questions-2.0/#use-case-15_2","text":"CQ: Which services are available for a certain task? SPARQL: SELECT ?service ?task WHERE { ?service rdf : type ?serviceType . ?serviceType rdfs : subClassOf * nfdicore : Service ; rdfs : label ?task . } CQ: Which softwares are available in a certain discipline? SPARQL: SELECT ?resource ?academicDisc WHERE { ?resource rdf : type nfdicore : Software ; nfdicore : subjectArea ?academicDisc . }","title":"Use Case 15"},{"location":"questions-2.0/#use-case-16_2","text":"CQ: Which organization offers a certain area of expertise? (IT) (including people in the organizations and their expertise) SPARQL: SELECT ?orga WHERE { ?orga rdf : type nfdicore : Organization . ?person nfdicore : member ?orga . { ?orga nfdicore : subjectArea ex : IT } UNION { ?person nfdicore : subjectArea ex : IT } } CQ: Which organization is a contact point for a certain service? SPARQL: SELECT distinct ?service ?agent WHERE { ?orga rdf : type nfdicore : Organization . ?service rdf : type ?serviceType . ?service nfdicore : contactPoint ?agent . ?serviceType rdfs : subClassOf * nfdicore : Service . { ?agent rdf : type nfdicore : Organization } UNION { ?person nfdicore : member ?agent } }","title":"Use Case 16"},{"location":"questions-2.0/#use-case-17_2","text":"CQ: What services in a certain academic discipline (computer science) are there? SPARQL: SELECT ?resource WHERE { ?resource rdf : type ?resourceType ; nfdicore : subjectArea ex : ComputerScience . ?resourceType rdfs : subClassOf * nfdicore : Service . }","title":"Use Case 17"},{"location":"questions-2.0/#use-case-23_1","text":"CQ: What are the best practices to publish a resource using a certain (open) licence? SPARQL: SELECT ?bestPractice ?license WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : license ?license . ?bestPractice bfo : RO_0000056 ?process ; rdf : type ?type . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"Use Case 23"},{"location":"questions-2.0/#use-case-42","text":"CQ: What controlled vocabularies can be utizlized (in the publishing process) of a resource? SPARQL: SELECT ?standard WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?resource nfdicore : standard ?standard . ?standard nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . } SPARQL: SELECT ?standard WHERE { ?process rdf : type bfo : IAO_0000444 ; bfo : OBI_0000299 ?resource . ?standard bfo : RO_0000056 ?process ; rdf : type ?type ; nfdicore : semanticExpressivity ?conVoc . ?conVoc rdf : type nfdicore : ControlledVocabulary . VALUES ?type { nfdicore : Specification nfdicore : Standard } }","title":"Use Case 42"},{"location":"questions/","text":"NFDIcore Competency Questions (CQs) and SPARQL Queries For the design and evaluation different projects contributed competecy questions the ontology should be able to answer. NFDI-MatWerk All NFDI-MatWerk Use Cases are available at the MatWerk Consortium Webpage Use Case 1 CQ 1: What are the specific services (in digitalization) that are used in a specific academic discipline? (data science) (need to be transferred to the new generation of scientists and professionals) CQ 2: What are the resources and events related to specific processes (demonstration and teaching)? CQ 3: What process are organisations involved in? and what are their roles in the processes? (PP participant universities). Use Case 2 CQ 1: What metadata schemas and ontologies are utilized in datasets that describe collections ? (materials used in Ni-based superalloys) CQ 2: What are the standard processes material data sets involved in? (the definition, identification and curation) CQ 3: How and which resources are data sets be connected to? ( detailed meta-data and context concerning materials history, data collection ) Use Case 03 CQ 2: What specific types of resources exist within a particular subject area, and what technological methods are employed in their storage?(Tomographic data from different techniques) CQ 2: What ontologies are used for resources in a specific subject area? SPARQL: CQ 3: What services are there for linkage and enrichment (data integration and access)? What software are they using? Use Case 04 CQ 1: What standards and specifications are used for data in a specific subject area? Use Case 05 CQ 1: What tools and services are used for storing and evaluation of a specific data type? CQ 2: What standards and specifications are used in a certain process? (data analysis) Use Case 06 CQ 1: What ontologies are used in a specific subject area? Use Case 08 CQ 1: What standards are used for resources in a specific process (standardization)? CQ 2: What guidelines are there for describing specific processes? (data analysis/visualization) Use Case 09 CQ 1: What data portals, services and software are contributed to a specific consortia? What are the subject area of these resources? Use Case 10 CQ 1: What is the funding organisation of a specific project? Use Case 12 CQ 1: What ontologies are used in resources contributed to specific consortia? What is their subject area? Use Case 13 CQ 1: What specific services (expert knowledge) are there and who is the contact point of these services? Use Case 15 CQ 1: Who is the contact point of a specific service? (ontology development) Use Case 16 CQ 1: What ontologies are used in resources and what is their subject area? Use Case 17 CQ 1: What ontologies are used in resources and what is their subject area? NFDI4DataScience All NFDI4DataScience Use Cases (Personas) are available at the NFDI4DataScience Consortium Webpage Use Case Alex CQ 1: What data participates in a specific task (event)? (data cleaning task) CQ 2: What services participate in a specific task (event)? ( (data cleaning services) Use Case Ben CQ 1: What documenting resources are related to a specific event (task)? Who is a contact point of the documentation process? Use Case Cassie CQ 1: What (training) services and events are available for specific data task (event)? Use Case David CQ 1: Who is a contact point of specific data (experimental)? What is their expertise? Use Case Emma CQ 1: List the events in the community, their description and their dates. Use Case Finn CQ 1: What are the (legal) guidelines for a specific process? (store sensitive data) Use Case Gina CQ 1: What are the relevant publications for a specific subject area or event (task)? Use Case Hassan CQ 1: What services are used in specific projects, which technological means and software are used in the services? If available, list guidelines/publications about these services. NFDI4Culture All NFDI4Culture Use Cases are available at the NFDI4Culture Consortium Webpage Use Case 1 CQ 1: What guidelines are available for a process (storing data) in a specific academic discipline (on musical performances)? CQ 2: What are the services and events related to specific processes ( structured filing and the handling of standard data)? Use Case 2 CQ 1: What standards are there for specific resource type (musical data)? Who is a contact point of the standards? CQ 2: What services are available for specific data task (event)? (keep different versions accessible) Use Case 3 CQ 1: What best practices are used in resources in a specific academic discipline? CQ 2: What are the best practices to publish a resource using in a certain academic discipline? CQ 3: What (training) events are available for specific ask (adequate handling of research data)? What is their timeframe? Use Case 4 CQ 1: What information about resource is used during the publication process? Use Case 5 CQ 1: What specific services are available for a certain task? (creating digital inventories of art collections) CQ 2: What licenses are used in the consortia for datasets with images? CQ 3: Who is a contact point of a specific service? CQ 4: What training events are available for a certain task (publishing and maintaining image data) Use Case 6 CQ 1: What are the best practices to publish a software in a certain academic discipline (digital humanities)? Use Case 7 CQ 1: What media types are available for resources in a certain academic discipline (music ethnology) and what are their licenses? CQ 2: What teaching and learning services are available in a certain academic discipline (music ethnology)? CQ 3: What guidelines are available (for an organization) to complete a process (publishing research data) for data in a specific academic discipline (performing arts)? Use Case 8 CQ 1: What processes (data publication, quality assurance, analyses) are certain resource types (e.g. questionnairs) in a certain academic discipline (e.g. musicology) involved in? Use Case 9 CQ 1: What are the best practices for publishing research data (resources)? Use Case 10 CQ: What organizations and persons are participating in processes related to creative works? Use Case 10 CQ 1: What organizations and persons are participating in processes related to creative works? Use Case 11 CQ 1: What resources that belong to a certain academic discipline (e.g. Art History) are available, what controlled vocabularies are associated with them? Use Case 12 CQ 1: What are the services and events related to specific processes (analysis, structured filing and the handling of standard data)? Use Case 14 CQ 1: What resources that belong to a certain academic discipline (e.g. Musicology) are available, what media types do they have and what qualities (e.g. semantic expressivity) are associated with them? Use Case 15 CQ 1: What resources that belong to a certain academic discipline (e.g. Archaeology) are available and represented by means of a certain semantic expressivity? CQ 2: What services for teaching and learning are available in a certain academic discipline? Use Case 16 CQ 1: What services for linking and enriching as well as publishing and disseminating research data are available in a certain academic discipline (eg. Architecture)? CQ 2: What collections are available in a certain geographical region (e.g. Germany)? Use Case 17 CQ 1: What services are available for linkage and enrichment as well as storage and preservation of research data? Use Case 18 CQ 1: Who has certain areas of expertise (computer science) and is a contact of a certain service (training and learning)? Use Case 20 CQ 1: What resource types are available in certain geographical locations? Use Case 21 CQ 1: What are the specifications and best practices utilized in the publishing process of a resource in a certain academic discipline (e.g. media science)? Use Case 22 CQ 1: What software or services are available for a certain task (event)? (versioning) Use Case 23 CQ 1: Which software is available in a certain academic discipline (music)? CQ 2: What learning and teaching services are available for resources of a certain discipline? Use Case 24 CQ 1: Which resources are related to a specific subject area (ex:Architecture)? Use Case 25 CQ 1: Which resources are related to an event that occurred in a specific time frame and which media type and, if available, semantic expressivity are they available in? Use Case 26 CQ 1: Which software was provided by an agent that participates in a project? CQ 2: Which resources and resource types were contributed by an agent that participates in a project? CQ 3: What persons participate in a project and belong to a certain academic discipline (computer science or digital humanities)? CQ 4: What learning and training services are contributed by an agent that participates in a project? Use Case 27 CQ 1: What specifications are there for certain types of resources and media types of resources? Use Case 28 CQ 1: What controlled vocabularies are available for certain resource types? NFDI4Memory All NFDI4Memory Use Cases are available at the NFDI4Memory Consortium Webpage Use Case 1 CQ 1: Who has certain areas of expertise (e.g. IT) and is involved in a certain service (training and education)? Use Case 3 CQ 1: What are the best practices for a certain resource? Use Case 5 CQ 1: What software is frequently used and who are their contact points? Use Case 6 CQ 1: What are the controlled vocabularies used in the consortium? CQ 2: What tools and services for a certain purpose are used in the consortium? Use Case 7 CQ 1: Which licences are used in certain data resources types? Use Case 8 CQ 1: What are the guidelines for a specific task? (publishing research data) Use Case 9 CQ 1: Which organizations (archives) located in Germany contribute certain resources and what licences do they use? CQ 2: What guidelines for data provenance for certain data type (archival) exist? Use Case 11 CQ 1: Which organization has members with expertise in computer science or information technology? Use Case 13 CQ 1: What data formats are used for resources in a certain academic discipline? Use Case 14 CQ 1: What licences given for resources in a certain media type? Use Case 15 CQ 1: Which services are available for a certain task? CQ 2: Which software is available in a certain discipline? Use Case 16 CQ 1: Which organization offers a certain area of expertise? (IT) (including people in the organizations and their expertise) CQ 2: Which organization is a contact point for a certain service? Use Case 17 CQ 1: What services in a certain academic discipline (computer science) are there? Use Case 23 CQ 1: What are the best practices to publish a resource using a certain (open) licence? Use Case 42 CQ 1: What controlled vocabularies can be utilized (in the publishing process) of a resource?","title":"NFDIcore Competency Questions (CQs) and SPARQL Queries"},{"location":"questions/#nfdicore-competency-questions-cqs-and-sparql-queries","text":"For the design and evaluation different projects contributed competecy questions the ontology should be able to answer.","title":"NFDIcore Competency Questions (CQs) and SPARQL Queries"},{"location":"questions/#nfdi-matwerk","text":"All NFDI-MatWerk Use Cases are available at the MatWerk Consortium Webpage","title":"NFDI-MatWerk"},{"location":"questions/#use-case-1","text":"CQ 1: What are the specific services (in digitalization) that are used in a specific academic discipline? (data science) (need to be transferred to the new generation of scientists and professionals) CQ 2: What are the resources and events related to specific processes (demonstration and teaching)? CQ 3: What process are organisations involved in? and what are their roles in the processes? (PP participant universities).","title":"Use Case 1"},{"location":"questions/#use-case-2","text":"CQ 1: What metadata schemas and ontologies are utilized in datasets that describe collections ? (materials used in Ni-based superalloys) CQ 2: What are the standard processes material data sets involved in? (the definition, identification and curation) CQ 3: How and which resources are data sets be connected to? ( detailed meta-data and context concerning materials history, data collection )","title":"Use Case 2"},{"location":"questions/#use-case-03","text":"CQ 2: What specific types of resources exist within a particular subject area, and what technological methods are employed in their storage?(Tomographic data from different techniques) CQ 2: What ontologies are used for resources in a specific subject area? SPARQL: CQ 3: What services are there for linkage and enrichment (data integration and access)? What software are they using?","title":"Use Case 03"},{"location":"questions/#use-case-04","text":"CQ 1: What standards and specifications are used for data in a specific subject area?","title":"Use Case 04"},{"location":"questions/#use-case-05","text":"CQ 1: What tools and services are used for storing and evaluation of a specific data type? CQ 2: What standards and specifications are used in a certain process? (data analysis)","title":"Use Case 05"},{"location":"questions/#use-case-06","text":"CQ 1: What ontologies are used in a specific subject area?","title":"Use Case 06"},{"location":"questions/#use-case-08","text":"CQ 1: What standards are used for resources in a specific process (standardization)? CQ 2: What guidelines are there for describing specific processes? (data analysis/visualization)","title":"Use Case 08"},{"location":"questions/#use-case-09","text":"CQ 1: What data portals, services and software are contributed to a specific consortia? What are the subject area of these resources?","title":"Use Case 09"},{"location":"questions/#use-case-10","text":"CQ 1: What is the funding organisation of a specific project?","title":"Use Case 10"},{"location":"questions/#use-case-12","text":"CQ 1: What ontologies are used in resources contributed to specific consortia? What is their subject area?","title":"Use Case 12"},{"location":"questions/#use-case-13","text":"CQ 1: What specific services (expert knowledge) are there and who is the contact point of these services?","title":"Use Case 13"},{"location":"questions/#use-case-15","text":"CQ 1: Who is the contact point of a specific service? (ontology development)","title":"Use Case 15"},{"location":"questions/#use-case-16","text":"CQ 1: What ontologies are used in resources and what is their subject area?","title":"Use Case 16"},{"location":"questions/#use-case-17","text":"CQ 1: What ontologies are used in resources and what is their subject area?","title":"Use Case 17"},{"location":"questions/#nfdi4datascience","text":"All NFDI4DataScience Use Cases (Personas) are available at the NFDI4DataScience Consortium Webpage","title":"NFDI4DataScience"},{"location":"questions/#use-case-alex","text":"CQ 1: What data participates in a specific task (event)? (data cleaning task) CQ 2: What services participate in a specific task (event)? ( (data cleaning services)","title":"Use Case Alex"},{"location":"questions/#use-case-ben","text":"CQ 1: What documenting resources are related to a specific event (task)? Who is a contact point of the documentation process?","title":"Use Case Ben"},{"location":"questions/#use-case-cassie","text":"CQ 1: What (training) services and events are available for specific data task (event)?","title":"Use Case Cassie"},{"location":"questions/#use-case-david","text":"CQ 1: Who is a contact point of specific data (experimental)? What is their expertise?","title":"Use Case David"},{"location":"questions/#use-case-emma","text":"CQ 1: List the events in the community, their description and their dates.","title":"Use Case Emma"},{"location":"questions/#use-case-finn","text":"CQ 1: What are the (legal) guidelines for a specific process? (store sensitive data)","title":"Use Case Finn"},{"location":"questions/#use-case-gina","text":"CQ 1: What are the relevant publications for a specific subject area or event (task)?","title":"Use Case Gina"},{"location":"questions/#use-case-hassan","text":"CQ 1: What services are used in specific projects, which technological means and software are used in the services? If available, list guidelines/publications about these services.","title":"Use Case Hassan"},{"location":"questions/#nfdi4culture","text":"All NFDI4Culture Use Cases are available at the NFDI4Culture Consortium Webpage","title":"NFDI4Culture"},{"location":"questions/#use-case-1_1","text":"CQ 1: What guidelines are available for a process (storing data) in a specific academic discipline (on musical performances)? CQ 2: What are the services and events related to specific processes ( structured filing and the handling of standard data)?","title":"Use Case 1"},{"location":"questions/#use-case-2_1","text":"CQ 1: What standards are there for specific resource type (musical data)? Who is a contact point of the standards? CQ 2: What services are available for specific data task (event)? (keep different versions accessible)","title":"Use Case 2"},{"location":"questions/#use-case-3","text":"CQ 1: What best practices are used in resources in a specific academic discipline? CQ 2: What are the best practices to publish a resource using in a certain academic discipline? CQ 3: What (training) events are available for specific ask (adequate handling of research data)? What is their timeframe?","title":"Use Case 3"},{"location":"questions/#use-case-4","text":"CQ 1: What information about resource is used during the publication process?","title":"Use Case 4"},{"location":"questions/#use-case-5","text":"CQ 1: What specific services are available for a certain task? (creating digital inventories of art collections) CQ 2: What licenses are used in the consortia for datasets with images? CQ 3: Who is a contact point of a specific service? CQ 4: What training events are available for a certain task (publishing and maintaining image data)","title":"Use Case 5"},{"location":"questions/#use-case-6","text":"CQ 1: What are the best practices to publish a software in a certain academic discipline (digital humanities)?","title":"Use Case 6"},{"location":"questions/#use-case-7","text":"CQ 1: What media types are available for resources in a certain academic discipline (music ethnology) and what are their licenses? CQ 2: What teaching and learning services are available in a certain academic discipline (music ethnology)? CQ 3: What guidelines are available (for an organization) to complete a process (publishing research data) for data in a specific academic discipline (performing arts)?","title":"Use Case 7"},{"location":"questions/#use-case-8","text":"CQ 1: What processes (data publication, quality assurance, analyses) are certain resource types (e.g. questionnairs) in a certain academic discipline (e.g. musicology) involved in?","title":"Use Case 8"},{"location":"questions/#use-case-9","text":"CQ 1: What are the best practices for publishing research data (resources)?","title":"Use Case 9"},{"location":"questions/#use-case-10_1","text":"CQ: What organizations and persons are participating in processes related to creative works?","title":"Use Case 10"},{"location":"questions/#use-case-10_2","text":"CQ 1: What organizations and persons are participating in processes related to creative works?","title":"Use Case 10"},{"location":"questions/#use-case-11","text":"CQ 1: What resources that belong to a certain academic discipline (e.g. Art History) are available, what controlled vocabularies are associated with them?","title":"Use Case 11"},{"location":"questions/#use-case-12_1","text":"CQ 1: What are the services and events related to specific processes (analysis, structured filing and the handling of standard data)?","title":"Use Case 12"},{"location":"questions/#use-case-14","text":"CQ 1: What resources that belong to a certain academic discipline (e.g. Musicology) are available, what media types do they have and what qualities (e.g. semantic expressivity) are associated with them?","title":"Use Case 14"},{"location":"questions/#use-case-15_1","text":"CQ 1: What resources that belong to a certain academic discipline (e.g. Archaeology) are available and represented by means of a certain semantic expressivity? CQ 2: What services for teaching and learning are available in a certain academic discipline?","title":"Use Case 15"},{"location":"questions/#use-case-16_1","text":"CQ 1: What services for linking and enriching as well as publishing and disseminating research data are available in a certain academic discipline (eg. Architecture)? CQ 2: What collections are available in a certain geographical region (e.g. Germany)?","title":"Use Case 16"},{"location":"questions/#use-case-17_1","text":"CQ 1: What services are available for linkage and enrichment as well as storage and preservation of research data?","title":"Use Case 17"},{"location":"questions/#use-case-18","text":"CQ 1: Who has certain areas of expertise (computer science) and is a contact of a certain service (training and learning)?","title":"Use Case 18"},{"location":"questions/#use-case-20","text":"CQ 1: What resource types are available in certain geographical locations?","title":"Use Case 20"},{"location":"questions/#use-case-21","text":"CQ 1: What are the specifications and best practices utilized in the publishing process of a resource in a certain academic discipline (e.g. media science)?","title":"Use Case 21"},{"location":"questions/#use-case-22","text":"CQ 1: What software or services are available for a certain task (event)? (versioning)","title":"Use Case 22"},{"location":"questions/#use-case-23","text":"CQ 1: Which software is available in a certain academic discipline (music)? CQ 2: What learning and teaching services are available for resources of a certain discipline?","title":"Use Case 23"},{"location":"questions/#use-case-24","text":"CQ 1: Which resources are related to a specific subject area (ex:Architecture)?","title":"Use Case 24"},{"location":"questions/#use-case-25","text":"CQ 1: Which resources are related to an event that occurred in a specific time frame and which media type and, if available, semantic expressivity are they available in?","title":"Use Case 25"},{"location":"questions/#use-case-26","text":"CQ 1: Which software was provided by an agent that participates in a project? CQ 2: Which resources and resource types were contributed by an agent that participates in a project? CQ 3: What persons participate in a project and belong to a certain academic discipline (computer science or digital humanities)? CQ 4: What learning and training services are contributed by an agent that participates in a project?","title":"Use Case 26"},{"location":"questions/#use-case-27","text":"CQ 1: What specifications are there for certain types of resources and media types of resources?","title":"Use Case 27"},{"location":"questions/#use-case-28","text":"CQ 1: What controlled vocabularies are available for certain resource types?","title":"Use Case 28"},{"location":"questions/#nfdi4memory","text":"All NFDI4Memory Use Cases are available at the NFDI4Memory Consortium Webpage","title":"NFDI4Memory"},{"location":"questions/#use-case-1_2","text":"CQ 1: Who has certain areas of expertise (e.g. IT) and is involved in a certain service (training and education)?","title":"Use Case 1"},{"location":"questions/#use-case-3_1","text":"CQ 1: What are the best practices for a certain resource?","title":"Use Case 3"},{"location":"questions/#use-case-5_1","text":"CQ 1: What software is frequently used and who are their contact points?","title":"Use Case 5"},{"location":"questions/#use-case-6_1","text":"CQ 1: What are the controlled vocabularies used in the consortium? CQ 2: What tools and services for a certain purpose are used in the consortium?","title":"Use Case 6"},{"location":"questions/#use-case-7_1","text":"CQ 1: Which licences are used in certain data resources types?","title":"Use Case 7"},{"location":"questions/#use-case-8_1","text":"CQ 1: What are the guidelines for a specific task? (publishing research data)","title":"Use Case 8"},{"location":"questions/#use-case-9_1","text":"CQ 1: Which organizations (archives) located in Germany contribute certain resources and what licences do they use? CQ 2: What guidelines for data provenance for certain data type (archival) exist?","title":"Use Case 9"},{"location":"questions/#use-case-11_1","text":"CQ 1: Which organization has members with expertise in computer science or information technology?","title":"Use Case 11"},{"location":"questions/#use-case-13_1","text":"CQ 1: What data formats are used for resources in a certain academic discipline?","title":"Use Case 13"},{"location":"questions/#use-case-14_1","text":"CQ 1: What licences given for resources in a certain media type?","title":"Use Case 14"},{"location":"questions/#use-case-15_2","text":"CQ 1: Which services are available for a certain task? CQ 2: Which software is available in a certain discipline?","title":"Use Case 15"},{"location":"questions/#use-case-16_2","text":"CQ 1: Which organization offers a certain area of expertise? (IT) (including people in the organizations and their expertise) CQ 2: Which organization is a contact point for a certain service?","title":"Use Case 16"},{"location":"questions/#use-case-17_2","text":"CQ 1: What services in a certain academic discipline (computer science) are there?","title":"Use Case 17"},{"location":"questions/#use-case-23_1","text":"CQ 1: What are the best practices to publish a resource using a certain (open) licence?","title":"Use Case 23"},{"location":"questions/#use-case-42","text":"CQ 1: What controlled vocabularies can be utilized (in the publishing process) of a resource?","title":"Use Case 42"},{"location":"versions/","text":"Versions Stable release versions The latest version of the ontology can always be found at: nfdicore.owl and nfdicore.ttl Variants The ontology is shipped in three varaints, each as OWL (*.owl) and Turtle serializations (*.ttl): full: nfdicore-full.ttl , nfdicore.ttl (default) base: nfdicore-base.ttl simple: nfdicore-simple.ttl The \"full release\" artefact contains all logical axioms, including inferred subsumptions. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. The \"base file\" is a specific release flavour. It reflects the intention of the ontology author for the official (publicly released) representation of the ontologies \"base entities\". \"Base entities\" are entities that are defined (\"owned\") by the ontology. The representation includes the intended public metadata (annotations), and classification (subClassOf hierarchy), including any statements where a base entity is the subject. The \"simple\" artefact only contains a simple existential graph of the terms defined in the ontology. This corresponds to the state before logical definitions and imports. For example, the only logical axioms are of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a class. The simple variant only contains the essential classes and no imports. The ontology \"main\" file nfdicore.ttl contains the full version. Editors' version Editors of this ontology should use the edit version. From this version all release variants are derived by the build workflows. Editors version: src/ontology/nfdicore-edit.owl Mappings Schema.org A mapping to selected concepts from Schema.org can be found in the file: mappings/schema.owl","title":"Versions"},{"location":"versions/#versions","text":"","title":"Versions"},{"location":"versions/#stable-release-versions","text":"The latest version of the ontology can always be found at: nfdicore.owl and nfdicore.ttl","title":"Stable release versions"},{"location":"versions/#variants","text":"The ontology is shipped in three varaints, each as OWL (*.owl) and Turtle serializations (*.ttl): full: nfdicore-full.ttl , nfdicore.ttl (default) base: nfdicore-base.ttl simple: nfdicore-simple.ttl The \"full release\" artefact contains all logical axioms, including inferred subsumptions. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. The \"base file\" is a specific release flavour. It reflects the intention of the ontology author for the official (publicly released) representation of the ontologies \"base entities\". \"Base entities\" are entities that are defined (\"owned\") by the ontology. The representation includes the intended public metadata (annotations), and classification (subClassOf hierarchy), including any statements where a base entity is the subject. The \"simple\" artefact only contains a simple existential graph of the terms defined in the ontology. This corresponds to the state before logical definitions and imports. For example, the only logical axioms are of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a class. The simple variant only contains the essential classes and no imports. The ontology \"main\" file nfdicore.ttl contains the full version.","title":"Variants"},{"location":"versions/#editors-version","text":"Editors of this ontology should use the edit version. From this version all release variants are derived by the build workflows. Editors version: src/ontology/nfdicore-edit.owl","title":"Editors' version"},{"location":"versions/#mappings","text":"","title":"Mappings"},{"location":"versions/#schemaorg","text":"A mapping to selected concepts from Schema.org can be found in the file: mappings/schema.owl","title":"Schema.org"},{"location":"odk-workflows/","text":"Default ODK Workflows Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Default ODK Workflows"},{"location":"odk-workflows/#default-odk-workflows","text":"Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Default ODK Workflows"},{"location":"odk-workflows/ContinuousIntegration/","text":"Introduction to Continuous Integration Workflows with ODK Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/nfdicore-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/nfdicore-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/ContinuousIntegration/#introduction-to-continuous-integration-workflows-with-odk","text":"Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/nfdicore-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/nfdicore-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/EditorsWorkflow/","text":"Editors Workflow The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future Local editing workflow Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc 1. Create issue Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change. 2. Update main branch In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull 3. Create feature branch Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess 4. Perform edit Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/nfdicore-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/nfdicore-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/nfdicore-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully. 4. Check the Git diff This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff 5. Quality control Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ). 5a. Local testing If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test 6. Pull request When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change). 7/5b. Continuous Integration Testing If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red. 8. Community review Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out! 9. Merge and cleanup When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request. 10. Changelog (Optional) It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#editors-workflow","text":"The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","text":"Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc","title":"Local editing workflow"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","text":"Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.","title":"1. Create issue"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","text":"In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull","title":"2. Update main branch"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","text":"Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess","title":"3. Create feature branch"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","text":"Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/nfdicore-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/nfdicore-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/nfdicore-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.","title":"4. Perform edit"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","text":"This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff","title":"4. Check the Git diff"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","text":"Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ).","title":"5. Quality control"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","text":"If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test","title":"5a. Local testing"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","text":"When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).","title":"6. Pull request"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","text":"If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.","title":"7/5b. Continuous Integration Testing"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","text":"Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!","title":"8. Community review"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","text":"When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.","title":"9. Merge and cleanup"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","text":"It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"10. Changelog (Optional)"},{"location":"odk-workflows/ManageAutomatedTest/","text":"Constraint violation checks We can define custom checks using SPARQL . SPARQL queries define bad modelling patterns (missing labels, misspelt URIs, and many more) in the ontology. If these queries return any results, then the build will fail. Custom checks are designed to be run as part of GitHub Actions Continuous Integration testing, but they can also run locally. Steps to add a constraint violation check: Add the SPARQL query in src/sparql . The name of the file should end with -violation.sparql . Please give a name that helps to understand which violation the query wants to check. Add the name of the new file to odk configuration file src/ontology/uberon-odk.yaml : Include the name of the file (without the -violation.sparql part) to the list inside the key custom_sparql_checks that is inside robot_report key. If the robot_report or custom_sparql_checks keys are not available, please add this code block to the end of the file. robot_report : release_reports : False fail_on : ERROR use_labels : False custom_profile : True report_on : - edit custom_sparql_checks : - name-of-the-file-check 3. Update the repository so your new SPARQL check will be included in the QC. sh run.sh make update_repo","title":"ManageAutomatedTest"},{"location":"odk-workflows/ManageAutomatedTest/#constraint-violation-checks","text":"We can define custom checks using SPARQL . SPARQL queries define bad modelling patterns (missing labels, misspelt URIs, and many more) in the ontology. If these queries return any results, then the build will fail. Custom checks are designed to be run as part of GitHub Actions Continuous Integration testing, but they can also run locally.","title":"Constraint violation checks"},{"location":"odk-workflows/ManageAutomatedTest/#steps-to-add-a-constraint-violation-check","text":"Add the SPARQL query in src/sparql . The name of the file should end with -violation.sparql . Please give a name that helps to understand which violation the query wants to check. Add the name of the new file to odk configuration file src/ontology/uberon-odk.yaml : Include the name of the file (without the -violation.sparql part) to the list inside the key custom_sparql_checks that is inside robot_report key. If the robot_report or custom_sparql_checks keys are not available, please add this code block to the end of the file. robot_report : release_reports : False fail_on : ERROR use_labels : False custom_profile : True report_on : - edit custom_sparql_checks : - name-of-the-file-check 3. Update the repository so your new SPARQL check will be included in the QC. sh run.sh make update_repo","title":"Steps to add a constraint violation check:"},{"location":"odk-workflows/ManageDocumentation/","text":"Updating the Documentation The documentation for NFDICORE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch. Editing the docs Changing content All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below) Deploy the documentation The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd nfdicore/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://ISE-FIZKarlsruhe.github.io/nfdicore/ 3. Just to double check, you can now navigate to your documentation pages (usually https://ISE-FIZKarlsruhe.github.io/nfdicore/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#updating-the-documentation","text":"The documentation for NFDICORE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch.","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","text":"","title":"Editing the docs"},{"location":"odk-workflows/ManageDocumentation/#changing-content","text":"All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below)","title":"Changing content"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","text":"The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd nfdicore/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://ISE-FIZKarlsruhe.github.io/nfdicore/ 3. Just to double check, you can now navigate to your documentation pages (usually https://ISE-FIZKarlsruhe.github.io/nfdicore/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Deploy the documentation"},{"location":"odk-workflows/ReleaseWorkflow/","text":"The release workflow The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following. Run a release with the ODK Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd nfdicore/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo). Review the release (Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (nfdicore.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): nfdicore.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! nfdicore-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at nfdicore-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR! Merge the main branch Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished). Create a GitHub release Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/ISE-FIZKarlsruhe/nfdicore/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the nfdicore.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done. Debugging typical ontology release problems Problems with memory When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here . Problems when using OBO format based tools Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open nfdicore-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#the-release-workflow","text":"The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","text":"Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd nfdicore/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).","title":"Run a release with the ODK"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","text":"(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (nfdicore.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): nfdicore.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! nfdicore-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at nfdicore-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!","title":"Review the release"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","text":"Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).","title":"Merge the main branch"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","text":"Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/ISE-FIZKarlsruhe/nfdicore/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the nfdicore.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done.","title":"Create a GitHub release"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","text":"","title":"Debugging typical ontology release problems"},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","text":"When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here .","title":"Problems with memory"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","text":"Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open nfdicore-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Problems when using OBO format based tools"},{"location":"odk-workflows/RepoManagement/","text":"Managing your ODK repository Updating your ODK repository Your ODK repositories configuration is managed in src/ontology/nfdicore-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this. Managing imports You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following. Add new import To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/nfdicore-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore.owl> Import(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/ro_import.owl>) Import(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory). Modify an existing import If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\". Remove an existing import To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/nfdicore-edit.owl . remove the id from your src/ontology/nfdicore-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file. Customise an import By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/nfdicore-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/nfdicore-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/nfdicore.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline. Add a component A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/nfdicore-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/nfdicore.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/nfdicore-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web. Adding a new component based on a ROBOT template Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/nfdicore-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-your-odk-repository","text":"","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","text":"Your ODK repositories configuration is managed in src/ontology/nfdicore-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this.","title":"Updating your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-imports","text":"You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following.","title":"Managing imports"},{"location":"odk-workflows/RepoManagement/#add-new-import","text":"To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/nfdicore-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore.owl> Import(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/ro_import.owl>) Import(<https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/nfdicore/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory).","title":"Add new import"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","text":"If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\".","title":"Modify an existing import"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","text":"To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/nfdicore-edit.owl . remove the id from your src/ontology/nfdicore-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file.","title":"Remove an existing import"},{"location":"odk-workflows/RepoManagement/#customise-an-import","text":"By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/nfdicore-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/nfdicore-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/nfdicore.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline.","title":"Customise an import"},{"location":"odk-workflows/RepoManagement/#add-a-component","text":"A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/nfdicore-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/nfdicore.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/nfdicore-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web.","title":"Add a component"},{"location":"odk-workflows/RepoManagement/#adding-a-new-component-based-on-a-robot-template","text":"Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/nfdicore-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Adding a new component based on a ROBOT template"},{"location":"odk-workflows/RepositoryFileStructure/","text":"Repository structure The main kinds of files in the repository: Release files Imports Components Release files Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here . Imports Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in NFDICORE Import URL Type bfo http://purl.obolibrary.org/obo/bfo/2020/notime/bfo.owl mirror ro http://purl.obolibrary.org/obo/ro.owl custom iao http://purl.obolibrary.org/obo/iao.owl custom swo https://raw.githubusercontent.com/allysonlister/swo/master/swo.owl custom obi http://purl.obolibrary.org/obo/obi.owl slme edam https://edamontology.org/EDAM_1.25.owl custom dcat http://www.w3.org/ns/dcat3 custom ## Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component. These are the components in NFDICORE Filename URL imports-edit.owl None nfdicore-main.owl None","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#repository-structure","text":"The main kinds of files in the repository: Release files Imports Components","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","text":"Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here .","title":"Release files"},{"location":"odk-workflows/RepositoryFileStructure/#imports","text":"Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in NFDICORE Import URL Type bfo http://purl.obolibrary.org/obo/bfo/2020/notime/bfo.owl mirror ro http://purl.obolibrary.org/obo/ro.owl custom iao http://purl.obolibrary.org/obo/iao.owl custom swo https://raw.githubusercontent.com/allysonlister/swo/master/swo.owl custom obi http://purl.obolibrary.org/obo/obi.owl slme edam https://edamontology.org/EDAM_1.25.owl custom dcat http://www.w3.org/ns/dcat3 custom ## Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component. These are the components in NFDICORE Filename URL imports-edit.owl None nfdicore-main.owl None","title":"Imports"},{"location":"odk-workflows/SettingUpDockerForODK/","text":"Setting up your Docker environment for ODK use One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/nfdicore-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/SettingUpDockerForODK/#setting-up-your-docker-environment-for-odk-use","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/nfdicore-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/UpdateImports/","text":"Update Imports Workflow This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here . Importing a new term Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically Declaring terms to be imported There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template Prot\u00e9g\u00e9-based declaration This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology. Using term files Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported. Using the custom import template This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/nfdicore-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file . Refresh imports If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B Using the Base Module approach Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/nfdicore-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#update-imports-workflow","text":"This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here .","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","text":"Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically","title":"Importing a new term"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","text":"There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template","title":"Declaring terms to be imported"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","text":"This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology.","title":"Prot\u00e9g\u00e9-based declaration"},{"location":"odk-workflows/UpdateImports/#using-term-files","text":"Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported.","title":"Using term files"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","text":"This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/nfdicore-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file .","title":"Using the custom import template"},{"location":"odk-workflows/UpdateImports/#refresh-imports","text":"If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Refresh imports"},{"location":"odk-workflows/UpdateImports/#using-the-base-module-approach","text":"Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/nfdicore-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Using the Base Module approach"},{"location":"odk-workflows/components/","text":"Adding components to an ODK repo For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/nfdicore-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Refresh your repo by running sh run update_repo . This will automatically (1) create a new file in src/ontology/components/ , (2) update the -edit file so that it imports https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/components/your-component-name.owl (the IRI of your new component), and (3) update the XML catalog file ( src/ontology/catalog-v001.xml ) to redirect that IRI to the file in the src/ontology/components directory, so that the new component can be found by tools such as Prot\u00e9g\u00e9 or ROBOT, when they load the -edit file. If your component is to be generated by some automated process, add a goal in your custom Makefile ( src/ontology/nfdicore.Makefile ) and make it perform any task needed to generate the component: $(COMPONENTSDIR)/your-component-name.owl: $(SRC) <Insert here the code to produce the component> If the component is to be generated from a ROBOT template, the ODK can generate the appropriate code for you. For that, when adding the component fo the ODK configuration file (step 2 above), explicitly indicate that the component should be derived from template(s) and list the source templates: components: products: - filename: your-component-name.owl use_template: true templates: - template1.tsv - template2.tsv In this example, the component will be derived from the templates found in src/templates/template1.tsv and src/templates/template2.tsv . Initial empty templates will automatically be generated when the repository is refreshed (step 3). Likewise, the ODK can generate the required code for the case where the component is to be derived from SSSOM mappings: components: products: - filename: your-component-name.owl use_mappings: true mappings: - my-mappings.sssom.tsv and for the case where the component is to be fetched from a remote resource: components: products: - filename: your-component-name.owl source: https://example.org/component-source.owl","title":"Adding components to an ODK repo"},{"location":"odk-workflows/components/#adding-components-to-an-odk-repo","text":"For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/nfdicore-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Refresh your repo by running sh run update_repo . This will automatically (1) create a new file in src/ontology/components/ , (2) update the -edit file so that it imports https://nfdi.fiz-karlsruhe.de/ontology/nfdicore/components/your-component-name.owl (the IRI of your new component), and (3) update the XML catalog file ( src/ontology/catalog-v001.xml ) to redirect that IRI to the file in the src/ontology/components directory, so that the new component can be found by tools such as Prot\u00e9g\u00e9 or ROBOT, when they load the -edit file. If your component is to be generated by some automated process, add a goal in your custom Makefile ( src/ontology/nfdicore.Makefile ) and make it perform any task needed to generate the component: $(COMPONENTSDIR)/your-component-name.owl: $(SRC) <Insert here the code to produce the component> If the component is to be generated from a ROBOT template, the ODK can generate the appropriate code for you. For that, when adding the component fo the ODK configuration file (step 2 above), explicitly indicate that the component should be derived from template(s) and list the source templates: components: products: - filename: your-component-name.owl use_template: true templates: - template1.tsv - template2.tsv In this example, the component will be derived from the templates found in src/templates/template1.tsv and src/templates/template2.tsv . Initial empty templates will automatically be generated when the repository is refreshed (step 3). Likewise, the ODK can generate the required code for the case where the component is to be derived from SSSOM mappings: components: products: - filename: your-component-name.owl use_mappings: true mappings: - my-mappings.sssom.tsv and for the case where the component is to be fetched from a remote resource: components: products: - filename: your-component-name.owl source: https://example.org/component-source.owl","title":"Adding components to an ODK repo"}]}